[{"content":" Databending is the artistic misuse of digital information. It draws its name from the practice of circuit bending \u0026ndash; a practice where childrens\u0026rsquo; toys, cheap keyboards and effects pedals are deliberately short-circuited by bending the circuit board to generate spontaneous and unpredictable sounds. Databending takes a similar approach to circuit bending, using software to intentionally disrupt the information contained within a file. There\u0026rsquo;s all kinds of different techniques, some involving deep hex editing of certain parts of a compression algorithm, but other methods are surprisingly simple. A hex editor (or binary file editor or byte editor) is a computer program that allows for manipulation of the fundamental binary data that constitutes a computer file. As technology becomes more complex, more glitches occur. A new term, \u0026ldquo;glitch art\u0026rdquo; has been coined for these glitches. An increasing number of artists have started to value these glitches and have come to insert them intentionally in whatever they are trying to create though many argue that the term glitch art can only be used for glitches that are unintentional. The different techniques of databending can broadly be divided into three categories:\nIncorrect editing: In incorrect editing, a file is edited using software intended for a different type of data. Reinterpretation: Reinterpretation is where a file is converted from one medium to another. Field of forced errors: In this type of databending known bugs in programs are exploited to force them to fail, usually while writing a file in the hope that the written file will be corrupted. ","date":"2022-08-24T13:15:46+05:30","permalink":"https://jhaakansha.github.io/p/data-bending/","title":"Data Bending"},{"content":" Big O is the used to decribe the efficiency of algorithms. Academics use big O, big θ and big Ω to decribe runtimes.\nBig O : Big O describes the upper bound on time. An algorithm that requires O(n) time can also be described as requiring O(n²), O(n³), O(2ⁿ) etc. The algorithm is at least as fast as any of these. Therefore, they are upper bounds on the runtime. For example, if x\u0026lt;13, it is also true that x\u0026lt;100, x\u0026lt;1000 etc. Big Ω : Big Ω is a similar concept but for lower bound. If the runtime is decribed as Ω(n), then it can also be described by Ω(log n) and Ω(1). But, it won\u0026rsquo;t be faster than those runtimes. Big θ : θ means both O and Ω. That is, an algorithm is θ(n) if its both O(n) and Ω(n). θ gives a tight bound on runtimes. In industry, the meaning of O and θ seem to have been merged. Industry\u0026rsquo;s meaning of big O is closer to what academics mean by θ. The runtime for an algorithm is described in three different ways: Best Case : If we consider the case of sorting an array, then if all the elements are equal, then quick sort will traverse the array only once giving the runtime O(n). This is the least runtime possible, hence it is the best case scenario. Worst Case : If we get really unlucky and the pivot is repeatedly the biggest element in the array then we have the largest possible runtime. This is the worst case scenario. Expected Case : Usually, the conditions considered above don\u0026rsquo;t happen. In general cases, the expected runtime for quick sort will be O(n log n). ","date":"2022-07-14T12:09:29+05:30","permalink":"https://jhaakansha.github.io/p/big-o/","title":"Big O"}]