[{"content":"Runtime: O(nlogn) Memory : O(1)\nHeap Sort is a comparison-based sorting technique based on binary heap data structure. It is an in-place algorithm. Its typical implementation is not stable, but can be made stable. Typically, it is 2-3 times slower than well-implemented quicksort due to lack of locality of reference.\nAdvantages of heapsort\nEfficiency : The time required to perform heap sort increases logarithmicallywhile other algorithms may grow exponentially slower as the number of items to sort increases. Memory usage : Memory usage is minimal because apart from what is necessary to hold the initial list of items to be sorted,it needs no additional memory space to work. Simplicity : It is simpler to understand than other sorting algorithm because it does not use advanced computer science concepts.\nHeapify\nIt is the process of creating a heap data structure from a binary tree represented using an array. It is used to create Min-Heap or Max-Heap.Start from the first index of the non-leaf node whose index is given by n/2 - 1. Heapify using recursion. Implementation of Heap Sort\nWhat are the two phases of heap sort? Array is converted into max heap. Highest element is removed and the remaining elements are used to create a new max heap. Which is better: Heapsort or Mergesort? Mergesort is slightly faster than heapsort but mergesort requires extra storage space. Depending on the requirement, one should be chosen. Why is heapsort better than selection sort?\nHeapsort is similar to selection sort, but with a better way to get the maximum element. It takes advantage os the heap data structure to get the maximum element in constant time. ","date":"2022-09-18T10:44:35+05:30","permalink":"https://jhaakansha.github.io/p/heap-sort/","title":"Heap Sort"},{"content":"Runtime: average : O(nlogn) worst : O(n²) Auxiliary Space : O(n) In quick sort, we pick a random element and partition the array, such that all numbers that are less than the partitioning element come before all elements greater than it.\nIf we repeatedly partition the array around an element, the array will eventually become sorted. However, as the partitioned element is not huaranteed to be the median, our sorting could be very slow.\nQuick Sort is not a stable algorithm. However, any sorting algorithm can be made stable by considering indexes as comparison parametres.\nImplementation of QuickSort\nWorst Case\nThe worst case occurs when the partition process always picks the greatest or the smallest element as the pivot. If we consider the partition startegy where the last element is always picked as a pivot, the worst case obviously occurs when the array is already sorted.\nBest Case\nThe best case occurs when the partition process always picks the middle element as the pivot.\n3 WAY QUICKSORT\nConsider an array which has many redundant elements. For example {1,2,3,6,6,8,9,9,0,0,0,7}, If we pick 6 as the pivot, we fix only one 6 and recursively process remaining occurrences. The idea of 3 way quicksort is to process all occurrences of the pivot and is based on Dutch National Flag algorithm.\nImplementation of 3 way quicksort using Dutch National Flag algorithm\nThe time complexity of this algorithm is O(nlogn) and the space complexity is o(logn).\nWhy is Quick Sort preferred over Merge Sort for sorting arrays?\nQuickSort is an in-place sort (i.e. it does not require any extra storage) whereas mergesort requires O(n) extra storage. Allocating and de-allocating the extra space used for mergesort increases the running time of the algorithm. Most practical implementations of QuickSort use randomized version. It has expected time complexity of O(nlogn). The worst case is possible in randomized version also, but worst case does not occur for a particular pattern (like sorted array) and works well in practice. QuickSort is also cache friendly sorting algorithm as it has good locality of reference when used for arrays. It is also tail recursive, therefore tail call optimization is done. ","date":"2022-09-17T13:18:51+05:30","permalink":"https://jhaakansha.github.io/p/quick-sort/","title":"Quick Sort"},{"content":"Runtime: O(nlogn) Auxiliary Space : O(n)\nMerge Sort divides the array in half, sorts each of those halves and then merges them back together. Each of these halves has the same sorting algorithm applied to it. Eventually, its like merging two single-element arrays. The merge method operates by copying all the elements from target array segment into a helper array,keeping track of where the start of the left and right halves should be. Then, iterate through helper, copying the smaller element from each half into the array.\nIts is a stable algorithm.\nImplemetation of MergeSort\nDRAWBACKS\nIt is slower compared to other sort algorithms for smaller tasks. It requires additional memory space of O(n). Merge sort goes through whole process even if array is sorted. ","date":"2022-09-17T13:18:43+05:30","permalink":"https://jhaakansha.github.io/p/merge-sort/","title":"Merge Sort"},{"content":"Best Case Runtime: O(n) Worst Case Runtime : O(n²)\nThe method for implementation of insertion sort is similar to the one we use to arrange our cards. It is a stable sorting algorithm. The steps followed are:\nIterate from arr[1] to arr[n] over the array. Compare the current element (key) to its predecessor. If the key element is smaller than its predecessor, compare it to the element before. Move the greater elements one position up to make space for the swapped element. Implementation of InsertionSort\nBINARY INSERTIONSORT This method uses binary search to find the proper location to insert the selected item at each iteration. In normal insertion, sorting takes O(i) time (at ith iteration) in worst case, this can be reduced to O(log i). The algorithm, as a whole, still has the worst case running time of O(n²).\nImplementation of binary InsertionSort\n","date":"2022-09-17T02:03:01+05:30","permalink":"https://jhaakansha.github.io/p/insertion-sort/","title":"Insertion Sort"},{"content":"Runtime: O(n²) Memory : O(1)\nFind the smallest element using a linear scan and move it to the front. Using linear scane, traverse the array from the second element onwards and find the least element in this sub-array. Swap the second element with the least element in the sub-array (which will the second least element in the array). Continue doing this until all elements are in place.\nThe default implementation of selection sort is not stable. Implementation of SelectionSort\n","date":"2022-09-11T14:20:15+05:30","permalink":"https://jhaakansha.github.io/p/selection-sort/","title":"Selection Sort"},{"content":"Runtime: O(n²) Memory : O(1)\n*In this article each n refers to the number of elements present in the array.\nIn BubbleSort, we start at the beginning of the array and swap the first two elements if the first is greater than the second. Then, we go to the next pair and so on, continuously making sweeps of the array until it is sorted.\nIt ia a stable algorithm, i.e, the relative positions of equivalent elements remains the same.\nImplementation of simple BubbleSort\nThe above method is not optimized it could be bettered by stopping the algorithm if the inner loop does not cause any swap.\nThe optimized approach will run a little slower than the original one if all the passes are made. However, in the best situation, the time complexity will be O(n) as opposed to the original which was (n²) in all circumstances.\nWorst Case : The worst case occurs when all the elements are arranged in descending order.\nTotal number of iterations = n - 1\nAt pass 1: comparisons = n - 1 swaps = n - 1\nAt pass 2: compartisons = n - 2 swaps = n - 2\nand so on until the number of comparisons is 1. Therefore, total number of comparisons required to sort the array\n= (n - 1) + (n - 2) + (n - 3) + \u0026hellip;. + 2 + 1\n= (n - 1)(n - 1 + 1)/2\n= n(n - 1)/2\nThis is equivalent to n².\nBest Case : The best occurs when the array is already sorted. The time complexity in this case is O(n).\nImplementation of optimized BubbleSort\nRECURSIVE BUBBLESORT\nThe following steps are followed to implement recursive bubblesort:-\nPlace the largest element at its position, this operation makes sure that the first largest element will be placed at the end of the array. Recursively call for the rest n - 1 elements with the same operation and place the next greater element at its position. The base condition for this recursion call would be when the number of elements in the array becomes 0 or 1 then, simply return. Implementation of recursive BubbleSort\n","date":"2022-09-11T12:27:42+05:30","permalink":"https://jhaakansha.github.io/p/bubble-sort/","title":"Bubble Sort"},{"content":" Databending is the artistic misuse of digital information. It draws its name from the practice of circuit bending \u0026ndash; a practice where childrens\u0026rsquo; toys, cheap keyboards and effects pedals are deliberately short-circuited by bending the circuit board to generate spontaneous and unpredictable sounds. Databending takes a similar approach to circuit bending, using software to intentionally disrupt the information contained within a file. There\u0026rsquo;s all kinds of different techniques, some involving deep hex editing of certain parts of a compression algorithm, but other methods are surprisingly simple. A hex editor (or binary file editor or byte editor) is a computer program that allows for manipulation of the fundamental binary data that constitutes a computer file. As technology becomes more complex, more glitches occur. A new term, \u0026ldquo;glitch art\u0026rdquo; has been coined for these glitches. An increasing number of artists have started to value these glitches and have come to insert them intentionally in whatever they are trying to create though many argue that the term glitch art can only be used for glitches that are unintentional. The different techniques of databending can broadly be divided into three categories:\nIncorrect editing: In incorrect editing, a file is edited using software intended for a different type of data. Reinterpretation: Reinterpretation is where a file is converted from one medium to another. Field of forced errors: In this type of databending known bugs in programs are exploited to force them to fail, usually while writing a file in the hope that the written file will be corrupted. ","date":"2022-08-24T13:15:46+05:30","permalink":"https://jhaakansha.github.io/p/data-bending/","title":"Data Bending"},{"content":" Big O is the used to decribe the efficiency of algorithms. Academics use big O, big θ and big Ω to decribe runtimes.\nBig O : Big O describes the upper bound on time. An algorithm that requires O(n) time can also be described as requiring O(n²), O(n³), O(2ⁿ) etc. The algorithm is at least as fast as any of these. Therefore, they are upper bounds on the runtime. For example, if x\u0026lt;13, it is also true that x\u0026lt;100, x\u0026lt;1000 etc. Big Ω : Big Ω is a similar concept but for lower bound. If the runtime is decribed as Ω(n), then it can also be described by Ω(log n) and Ω(1). But, it won\u0026rsquo;t be faster than those runtimes. Big θ : θ means both O and Ω. That is, an algorithm is θ(n) if its both O(n) and Ω(n). θ gives a tight bound on runtimes. In industry, the meaning of O and θ seem to have been merged. Industry\u0026rsquo;s meaning of big O is closer to what academics mean by θ. The runtime for an algorithm is described in three different ways: Best Case : If we consider the case of sorting an array, then if all the elements are equal, then quick sort will traverse the array only once giving the runtime O(n). This is the least runtime possible, hence it is the best case scenario. Worst Case : If we get really unlucky and the pivot is repeatedly the biggest element in the array then we have the largest possible runtime. This is the worst case scenario. Expected Case : Usually, the conditions considered above don\u0026rsquo;t happen. In general cases, the expected runtime for quick sort will be O(n log n). ","date":"2022-07-14T12:09:29+05:30","permalink":"https://jhaakansha.github.io/p/big-o/","title":"Big O"}]