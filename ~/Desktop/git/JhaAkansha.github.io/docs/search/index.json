[{"content":"⚡ ByteBattles with Ather Energy: 30 Hours, One EV \u0026amp; a Lot of Coffee “When was the last time you coded on a real electric vehicle?”\n– Probably never. Until ByteBattles.\n🚀 What is ByteBattles? ByteBattles is a national-level firmware hackathon hosted by Ather Energy, inviting students and professionals from across India to tackle real-world challenges in the EV (Electric Vehicle) space.\nParticipants were challenged to develop firmware solutions for the Ather 450x, a flagship electric scooter—and yes, we were actually given the scooter to code on.\n✈️ Pune to Bangalore: The Journey Begins 🕒 3:00 AM: Flight from Pune 🕗 8:00 AM: Land in Bangalore and reach Ather HQ ☕ 8:30 AM: Check-in, breakfast, orientation 🎽 Swag secured: T-shirts + ID cards 🏢 Office tour: Our base for the next 30 hours We were five caffeine-powered engineers, barely awake but buzzing with excitement.\n🧠 The Problem Statement At the orientation, we were introduced to the real challenge:\nImplement 7 firmware features on the Ather 450x over the weekend.\nWith access to development boards, firmware libraries, and the Ather 450x, we were off to a flying start. Kind of.\n💥 Hack Mode: ON 11:00 AM: Coding begins ⌨️ Problem 1 solved in 10 mins—or so we thought 😓 Turns out\u0026hellip; it was a faulty USB cable While the volunteers helped debug the hardware, we brainstormed and coded two more features—with no working hardware in hand. Grit.\n😵 Debug, Debug, Repeat Two team members: Locked into debugging mode Rest of us: Shipping code like our lives depended on it Result: 4 features implemented by the end of the round But we missed the early submission points—by just a few minutes. That sealed our fate: we didn’t make it to Phase 2.\n🧩 Phase 2: Watching from the Sidelines While the top 15 teams moved upstairs to tackle advanced problem statements, we made the most of our time:\n🤝 Networked with students \u0026amp; working pros 🎤 Interactive Q\u0026amp;As with Ather engineers 🛵 Explored stripped-down Ather 450x models 🎉 Activities: Dumb charades, music, DJ, even a mystery guitar jam session 💤 Night Mode: Floor is the New Bed By 3 AM, the office had turned into a co-living space:\n🛏️ Bean bags, sleeping bags, and even just floors 💤 Brain fog, but hearts still racing with excitement ⚙️ The Plot Twist The next morning, we got another shot at accessing our EV.\n30 minutes later, we solved the bug that had blocked us the entire day before.\nIn one shot, 3 additional features started working.\nTalk about tragic timing.\n🎯 Final Takeaways Despite not making it to the final round, ByteBattles was:\n💡 A rare opportunity to work with real EV hardware 🔧 Our first hands-on experience with firmware development 🤝 A brilliant space to network with passionate engineers 📚 A steep learning curve that pushed all our boundaries ","date":"2025-04-08T13:23:44+05:30","permalink":"https://jhaakansha.github.io/p/bytebattles-2.0/","title":"ByteBattles 2.0"},{"content":"Salt Typhoon: China\u0026rsquo;s Covert Cyber Espionage Campaign In the shadows of global cyberspace, a formidable threat actor known as Salt Typhoon has emerged, orchestrating sophisticated cyber espionage operations with alarming precision. This Chinese state-sponsored group, also referred to as GhostEmperor, FamousSparrow, and UNC2286, has been linked to China\u0026rsquo;s Ministry of State Security (MSS) (home.treasury.gov).\n🌀 Origins and Evolution Salt Typhoon\u0026rsquo;s activities date back to at least August 2019, with early attempts to infiltrate high-profile targets, including former President Donald Trump (home.treasury.gov). By late 2024, the group had escalated its operations, breaching major U.S. telecommunications companies such as Verizon, AT\u0026amp;T, and T-Mobile (bleepingcomputer.com). These attacks compromised sensitive data, including call metadata and, in some instances, audio recordings of high-profile individuals (bleepingcomputer.com).\n🛠️ Tactics and Tools Salt Typhoon employs a diverse arsenal of tools to infiltrate and maintain access to targeted networks. Notably, they utilize Demodex, a Windows kernel-mode rootkit, to gain remote control over servers (home.treasury.gov). Their toolkit includes:\nBITSAdmin and CertUtil: For downloading and executing malicious payloads. PowerShell scripts: For reconnaissance and lateral movement. SparrowDoor: A custom backdoor facilitating persistent access. Malleable C2: For command and control communication. These tools enable Salt Typhoon to operate stealthily, exfiltrating vast amounts of sensitive information over extended periods (home.treasury.gov).\n🌐 Global Impact Salt Typhoon\u0026rsquo;s operations are not confined to the United States. The group has targeted telecommunications companies across dozens of countries, exploiting vulnerabilities in core network components, including routers manufactured by Cisco (bleepingcomputer.com). Their activities have raised significant concerns about the security of global communication infrastructures.\n🧩 Strategic Objectives The group\u0026rsquo;s primary focus appears to be counterintelligence, aiming to monitor and intercept communications of government officials and high-profile individuals. By compromising telecommunications infrastructure, Salt Typhoon gains access to a wealth of sensitive information, which can be leveraged for strategic advantage in geopolitical contexts (home.treasury.gov).\n⚖️ International Response In response to these cyber intrusions, the U.S. Department of the Treasury sanctioned Sichuan Juxinhe Network Technology, a Shanghai-based cybersecurity firm alleged to be directly involved with Salt Typhoon (home.treasury.gov). Additionally, the White House has issued advisories to assist system administrators in hardening network security to mitigate potential threats from such advanced persistent threats (bleepingcomputer.com).\n🔍 Conclusion Salt Typhoon exemplifies the evolving nature of cyber espionage, where state-sponsored actors employ sophisticated tactics to infiltrate critical infrastructure and exfiltrate sensitive data. As cyber threats become increasingly complex and pervasive, it is imperative for organizations worldwide to bolster their cybersecurity measures and remain vigilant against such advanced persistent threats.\n","date":"2025-03-17T13:23:49+05:30","permalink":"https://jhaakansha.github.io/p/salt-typhoon/","title":"Salt Typhoon"},{"content":"🏎️ Fueling Victory: How Formula One Uses Big Data to Win Races Formula One might look like a sport where victory comes down to the fastest car or most daring driver — but behind every blistering lap is an enormous engine of data. Welcome to the world where machine learning meets motorsport, and terabytes of real-time information help shape every twist, turn, and pit stop decision.\nIn this post, we’ll walk you through how Formula One teams use the big data lifecycle to gain a competitive edge — from sensor-packed cars and telemetry streams to predictive modeling and post-race analysis.\n📥 1. Data Collection: The Stream Begins F1 cars are, quite literally, data machines on wheels.\nSensors Everywhere: Each car carries around 300 sensors, constantly measuring temperature, pressure, tire wear, fuel consumption, g-forces, and more — generating over 1.1 million data points per second. Telemetry: All of this data is transmitted in real time to the pit wall and team HQ. Teams monitor engine health, lap times, braking patterns, and tire degradation live as the race unfolds. Track and Weather Data: Sensors around the circuit track ambient and surface temperature, wind speed, and grip levels. Historical Performance: Teams also dig deep into archives of race history — weather patterns, past pit strategies, driver behaviors — to inform current decisions. \u0026ldquo;Every lap is a lesson — and the data is the teacher.\u0026rdquo;\n🗄️ 2. Data Storage: Where All That Info Lives Cloud Data Lakes: With terabytes of data per race weekend, teams rely on cloud infrastructure and data lakes for scalable storage and quick access. Relational Databases: Structured data like lap times, split timings, and car setup parameters are stored in SQL-based databases. Time-Series Databases: For telemetry and sensor data streaming second-by-second, time-series databases like InfluxDB are key. This combination ensures structured and unstructured data are both accessible for real-time and post-race analysis.\n🧹 3. Data Cleaning \u0026amp; Preprocessing: Removing the Noise Before the magic happens, data needs polishing.\nNoise Reduction: Filtering out sensor glitches or temporary drops in telemetry due to connection issues. Missing Data Handling: Using techniques like interpolation to fill in gaps when sensors fail. Data Normalization: Aligning units and formats so that telemetry, weather, and driver data can be compared or combined. Clean data ensures accurate insights and decisions — especially when the wrong move can cost a race.\n📊 4. Data Analysis: Understanding the Race Once clean, the data goes through layers of analytics:\nDescriptive Analytics: What happened? Analyzing trends in lap times, tire wear, or driver performance. Predictive Analytics: What might happen? Machine learning models forecast tire degradation, fuel usage, and even opponent strategies. Prescriptive Analytics: What should we do? Data-driven suggestions for pit strategy, tire choice, or engine tuning. Real-Time Analysis: Engineers monitor live dashboards to react instantly — like changing pit strategy when a rival undercuts. This is where the data starts driving decisions.\n📺 5. Data Visualization: Making It All Visible In the heat of a race, engineers don’t have time to read logs.\nDashboards: Real-time visualizations help the pit crew monitor performance metrics like tire temp, brake health, and lap deltas. Graphs \u0026amp; Heatmaps: Show trends like rising tire wear or engine stress over time. 3D Simulations: Entire race strategies can be modeled visually, showing how a pit stop now could play out 20 laps later. Tools like McLaren Applied\u0026rsquo;s F1 Tempo give teams a way to translate raw data into race-ready insights.\n🤖 6. Machine Learning \u0026amp; Model Refinement F1 teams are now fully embracing AI.\nModel Training: As new data flows in every race, models are retrained to better predict outcomes like undercut potential or optimal tire stint lengths. AI Simulation: Simulated races using neural networks help optimize strategies across hundreds of “what-if” scenarios. Continuous Learning: Models improve with every season, learning from what worked — and what didn’t. 🧠 7. Decision-Making: Turning Data into Action Strategic Adjustments: Mid-race decisions like undercutting an opponent or stretching tire life are made based on predictive analysis. Driver Feedback: Data is sent directly to the driver’s cockpit, offering guidance on tire performance, optimal lines, and gaps to rivals. Risk Mitigation: If a part is close to failure, predictive alerts can warn the crew before disaster strikes. Car Setup Optimization: In practice sessions, teams tweak setups — suspension, aero balance, etc. — based on how the data aligns with driver feel. 🔁 8. Feedback Loop: Post-Race Analysis After the checkered flag, the data work is far from over.\nPost-Race Review: Comparing expected vs. actual performance, pit strategy efficiency, and any anomalies. Driver Insights: Their feedback — combined with telemetry — helps fine-tune future car setups. Continuous Improvement: Learnings feed into the next race, improving everything from brake cooling systems to fuel maps. 🚀 9. What’s Next: Advanced Analytics in F1 Formula One is only accelerating its use of advanced data techniques:\nAI-Driven Strategy Engines: Predict outcomes based on live conditions and adjust strategies automatically. Predictive Maintenance: AI can now predict part failures before they happen, saving races (and millions in repairs). Biometric Analysis: Some teams are exploring heart rate, hydration levels, and fatigue in drivers to optimize performance and safety. 🏁 Final Thoughts Formula One is no longer just a test of speed — it’s a test of data mastery. Every lap, every pit stop, every millisecond is backed by mountains of analysis and predictive modeling.\nIn this race, the car is fast. The driver is skilled.\nBut data is the difference.\nWant to dive deeper? Explore tools like F1 Tempo by McLaren Applied or learn how AI is reshaping motorsport.\n","date":"2025-02-14T13:23:53+05:30","permalink":"https://jhaakansha.github.io/p/understanding-big-data-analytics-with-formula-one/","title":"Understanding Big Data Analytics with Formula One"},{"content":"🎨 My Internship Experience at Adobe \u0026ldquo;A dream internship, real-world impact, and a journey into the heart of creativity and innovation.\u0026rdquo;\nLanding an internship at Adobe was nothing short of surreal. It was a thrilling and unexpected milestone, and I couldn’t wait to see what life beyond the classroom looked like—working alongside industry veterans and building features that millions might use.\n🧑‍💻 Team Photoshop Express – Android Squad I joined the Photoshop Express team, where I primarily worked on the Android app. What amazed me from day one was the ownership I was given. I wasn\u0026rsquo;t just there to shadow someone—I was contributing to real features that impacted actual users.\n⏳ Tackling Frustration with a Wait State Loader 🔍 The Problem Long image processing times were leading to:\nUser frustration and app abandonment Confusion among new users trying to understand text-to-image generation Lost opportunities to highlight community creativity 💡 The Solution Introduce a wait state loader that:\nShows community-generated images Displays prompts used to create them Lets users copy prompts for inspiration while they wait ⚙️ What I Built ✅ An automated image slider that:\nPulls in community images from a remote server Displays them with clean, styled captions Allows quick one-tap prompt copying 🎥 Demo ✨ Leading the Generative Expand Feature \u0026ldquo;From concept to production—owning one of the most powerful AI-driven features in the app.\u0026rdquo;\n📸 What is Generative Expand? Generative Expand is an innovative feature that allows users to change the aspect ratio of an image by using AI to seamlessly generate and fill in the extended canvas based on the original content.\nImagine expanding a photo horizontally, and the app fills in the sides with realistic, context-aware details—automatically.\n🧑‍💻 My Role I was entrusted with end-to-end ownership of this feature during my internship, and it quickly became the most exciting and challenging part of my journey.\nResponsibilities: Architected the workflow for canvas resizing + AI content generation Integrated Firefly model capabilities into the Android app Ensured a smooth and intuitive UX, minimizing user friction Collaborated with designers, QA, and product teams for polish ⏳ Timeline \u0026amp; Execution ⏱️ 3 weeks of development 💬 Continuous feedback and iteration with mentors 🔍 Meticulous testing across devices and screen sizes ✅ Final implementation went live in the production app Seeing my feature shipped and used by real users was one of the most fulfilling moments of the internship.\nHere\u0026rsquo;s a demo video 🧠 Enhancing the Firefly Model through Prompt Engineering \u0026ldquo;The art of talking to AI—shaping how it sees, creates, and imagines.\u0026rdquo;\nOne of the most intellectually rewarding parts of my internship involved working on Prompt Engineering for Adobe’s Firefly model, which powers several upcoming features in the Photoshop Express iOS app.\n🔍 The Challenge The Firefly model needed smarter, trend-aware inputs to produce cleaner, more realistic images. That meant:\nReducing visual noise Avoiding confusing or ambiguous prompts Keeping up with rapidly evolving creative trends 🛠️ What I Did As a Prompt Engineer, my responsibilities included:\n🧼 Refining existing prompts to improve clarity and output quality 🆕 Creating new prompt categories based on trend analysis ✂️ Cleaning up noisy or redundant text elements 🔍 Studying competitor apps + user-generated galleries for inspiration This wasn’t just a technical task—it was part research, part design thinking, and part creative strategy.\n📈 The Results ✅ The model produced cleaner, more accurate, and visually appealing images\n✅ New prompt categories helped Firefly stay aligned with creative trends\n✅ Prompt templates became more user-friendly and intuitive\nI wasn’t just teaching AI how to create—I was helping it create better.\n🎯 Wrapping It All Up ✨ Conclusion My internship at Adobe was a transformative journey. I got to:\n💡 Build real features that shipped to production 🤖 Work hands-on with AI-driven technologies 📱 Contribute directly to Photoshop Express on Android and iOS 🤝 Collaborate with an incredibly talented and welcoming team It was the perfect blend of challenge, creativity, and real-world impact.\nI walked in as a student curious about how the industry works—and walked out as a developer who\u0026rsquo;s contributed to one of the most recognized creative platforms in the world.\n🙏 Final Thanks A huge thank you to my mentors and the amazing team at Adobe Photoshop Express. You believed in me, challenged me, and gave me the space to grow. I’ll carry this experience with me for a long time.\n","date":"2024-08-12T13:23:39+05:30","permalink":"https://jhaakansha.github.io/p/more-than-just-photoshop-my-adobe-internship-experience/","title":"More Than Just Photoshop: My Adobe Internship Experience"},{"content":"ChatGPT is a conversational AI that listens, learns and challenges developed by OpenAI. It was launched in November 2022. The model is based on Generative Pretrained Transformer 3 (GPT 3) architecture, which is the largest and most advanced models to date. It has been fine-tuned by both supervised and reinforced machine learning techniques. ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.\nTraining method The model was using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. An initial model was trained using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. The trainers were given access to model-written suggestions to help them compose their responses. This new dialogue dataset was mixed with the InstructGPT dataset, which was then transformed into a dialogue format. To create a reward model for reinforcement learning, it was necessary to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, conversations that AI trainers had with the chatbot were taken. A model-written message was randomly selected, several alternative completions were sampled, and AI trainers ranked them. Using these reward models the model can be fine-tuned using Proximal Policy Optimization. Several iterations of this process were performed to perfect the chatbot.\nChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022.\nLimitations ChatGPT sometimes writes plausible-sounding but incorrect or nonsensical answers. Fixing this issue is challenging, as: (1) during RL training, there’s currently no source of truth; (2) training the model to be more cautious causes it to decline questions that it can answer correctly; and (3) supervised training misleads the model because the ideal answer depends on what the model knows, rather than what the human demonstrator knows. ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For example, given one phrasing of a question, the model can claim to not know the answer, but given a slight rephrase, can answer correctly. The model is often excessively verbose and overuses certain phrases, such as restating that it’s a language model trained by OpenAI. These issues arise from biases in the training data (trainers prefer longer answers that look more comprehensive) and well-known over-optimization issues. Ideally, the model would ask clarifying questions when the user provided an ambiguous query. Instead, the current models usually guess what the user intended. While we’ve made efforts to make the model refuse inappropriate requests, it will sometimes respond to harmful instructions or exhibit biased behavior. Currently, the Moderation API is being used to warn or block certain types of unsafe content, but it is expected to have some false negatives and positives for now. ","date":"2023-03-20T04:49:14+05:30","permalink":"https://jhaakansha.github.io/p/gpt-4.0/","title":"GPT 4.0"},{"content":"A queue is an ordered list in which insertions are done at one end (rear) and deletions are done at the other end (front). The first element to be inserted is the first element to be deleted, i.e. it follows FIFO (First In First Out) concept. Circular linked lists and circular arrays are used to implement queues.\nMain queue operations Enqueue: Inserts an element at the end of the queue. Dequeue: Removes and return the element at the front of the queue. Auxiliary queue operations Front: Returns the element at the front without removing it. QueueSize(): Returns the number of elements stored. IsEmptyQueue: Indicates whether no elements are stored. Simple Circular Array Implementation Elements are added circularly and two variables are used to keep track of the start element (front) and the element at the end (rear).\nThe array may become full and an enqueue operation will throw a full queue exception. Similarly, deleting an element will throw an empty queue exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 struct arrayQueue { int front, rear; int capacity; int *array; }; struct arrayQueue *Queue(int size) { struct arrayQueue *Q = malloc(sizeof(structarrayQueue)); if (!Q) { return NULL; } Q-\u0026gt;capacity = size; Q-\u0026gt;front = Q-\u0026gt;rear = -1; Q-\u0026gt;array = malloc(Q-\u0026gt;capacity*sizeof(int)); if (!Q-\u0026gt;array) { return NULL; } return Q; } int isEmptyQueue(struct arrayQueue *Q) { return (Q-\u0026gt;front == -1); } int isFullQueue(struct arrayQueue *Q) { return( ((Q-\u0026gt;rear+1) % (Q-\u0026gt;capacity)) == Q-\u0026gt;front); } int queueSize() { return ((Q-\u0026gt;capacity - Q-\u0026gt;front + Q-\u0026gt;rear +1) % Q-\u0026gt;capacity) ; } void Enqueue(struct arrayQueue *Q, int data) { if (isFullQueue(Q)) { printf(\u0026#34;Queue Overflow\u0026#34;); } else { Q-\u0026gt;rear = (Q-\u0026gt;rear +1)% Q-\u0026gt;capacity; Q-\u0026gt;array[Q-\u0026gt;rear] = data; if (Q-\u0026gt;front == -1) { Q-\u0026gt;front = Q-\u0026gt;rear; } } } int Dequeue(struct arrayQueue *Q) { int data = 0; if (isEmptyQueue(Q)) { printf(\u0026#34;Queue is empty\u0026#34;); return 0; } else { data = Q-\u0026gt;array[Q-\u0026gt;front]; if (Q-\u0026gt;front = Q-\u0026gt;rear) { Q-\u0026gt;front = Q-\u0026gt;rear = -1; } else { Q-\u0026gt;front = (Q-\u0026gt;front+1) % Q-\u0026gt;capacity; } } return data; } void deleteQueue(struct arrayQueue *Q) { if (Q) { if (Q-\u0026gt;array) { free(Q-\u0026gt;array); } free(Q); } } Limitations The maximum size of queue must be defined at the beginning and cannot be changed. Trying to enqueue a new element into a full queue causes an implementation- specific exception.\nDynamic Circular Array Implementation In the dynamic array implemntation of queue, the size of queue gets doubled in size each time the enqueue operation is called on a full queue.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 struct DynArrayQueue { int front, rear; int capacity; int *array; }; struct DynArrayQueue * createQueue() { struct DynArrayQueue *Q = malloc(sizeof(struct DynArrayQueue)); if (!Q) { return NULL; } Q-\u0026gt;capacity = 1; Q-\u0026gt;front = Q-\u0026gt;rear = -1; Q-\u0026gt;array = malloc(Q-\u0026gt;capacity*sizeof(int)); if (!Q-\u0026gt;array) { return NULL; } return Q; } int isEmptyQueue(struct DynArrayQueue *Q) { return (Q-\u0026gt;front == -1); } int isFullQueue(struct DynArrayQueue *Q) { return ((Q-\u0026gt;rear + 1) % Q-\u0026gt; capacity == Q-front); } int queueSize() { return((Q-\u0026gt;capacity - Q-\u0026gt;front + Q-\u0026gt;rear + 1) % Q-\u0026gt;capacity); } void Enqueue(struct DynArrayQueue *Q, int data) { if(isFullQueue(Q)) { resizeQueue(Q); } } void resizeQueue(struct DynArrayQueue *Q) { int size = Q-\u0026gt;capacity; Q-\u0026gt;capacity = Q-\u0026gt;capacity*2; Q-\u0026gt;array = realloc(Q-\u0026gt;array, Q-\u0026gt;capacity); if (!q-\u0026gt;array) { printf(\u0026#34;Memory error\u0026#34;); return; } if (Q-\u0026gt;front \u0026gt; Q-\u0026gt;rear) { for (int i = 0; i\u0026lt;Q-\u0026gt;front; i++) { Q-\u0026gt;array[i+size] = Q-\u0026gt;array[i]; } Q-\u0026gt;rear = Q-\u0026gt;rear + size; } } int Dequeue(struct DynArrayQueue *Q) { int data = 0; if (isEmptyQueue(Q)) { printf(\u0026#34;Queue is empty\u0026#34;); return 0; } else { data = Q-\u0026gt;array[Q-\u0026gt;front]; if (Q-\u0026gt;front == Q-\u0026gt;rear) { Q-\u0026gt;front = Q-\u0026gt;rear = -1; } else { Q-\u0026gt;front = (Q-\u0026gt;front + 1) % Q-\u0026gt;capacity; } } return data; } void deleteQueue(struct DynArrayQueue *Q) { if (Q) { if (Q-\u0026gt;array) { free(Q-\u0026gt;array); } free(Q-\u0026gt;array) //CROSS CHECK THIS ONCE } } LInked List Implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 struct ListNode { int data; struct ListNode *next; }; struct queue { struct ListNode *front; struct ListNode *rear; }; struct queue *createQueue() { struct queue *Q; struct ListNode *temp; Q = malloc(sizeof(struct queue)); if (!Q) { return NULL; } temp = malloc(sizeof(struct ListNode)); Q-\u0026gt;front = Q-\u0026gt;rear = NULL; return Q; } int isEmptyQueue (struct queue *Q) { return (Q-\u0026gt;front == NULL); } void Enqueue (struct queue *Q, int data) { struct ListNode *newNode; newNode = malloc(sizeof(struct ListNode)); if (!newNode) { return NULL; } newNode-\u0026gt;data = data; newNode-\u0026gt;next = NULL; Q-\u0026gt;rear-\u0026gt;next = newNode; Q-\u0026gt;rear = newNode; if (Q-\u0026gt;front == NULL) { Q-\u0026gt;front = Q-\u0026gt;rear; } } int Dequeue(struct queue *Q) { int data = 0; struct ListNode *temp; if (isEmptyQueue(Q)) { printf(\u0026#34;Queue is empty\u0026#34;); return 0; } else { temp = Q-\u0026gt;front; data = Q-\u0026gt;front-\u0026gt;data; Q-\u0026gt;front = Q-\u0026gt;front-\u0026gt;next; free(temp); } return data; } void deleteQueue(struct queue *Q) { struct ListNode *temp; while (Q) { temp = Q; Q = Q-\u0026gt;next; free(temp); } free(Q); } ","date":"2022-11-27T13:23:35+05:30","permalink":"https://jhaakansha.github.io/p/queue/","title":"Queue"},{"content":"CREATE REPOSITORIES New repostiories can be created either locally or by copying a file that alredy exists on GitHub.\n1 2 $ git init - Turn an existing directory into git repository. $ git clone [url] - Clone a repository that already exists in GitHub including all the files, branches and commits. CONFIGURE Configure user information for all local repositories.\n1 2 3 $ git config --global user.name \u0026#34;[name]\u0026#34; - Sets the name you want attached to your commit transactions. $ git config --global user.email \u0026#34;[email-address]\u0026#34; - Sets the email you want attached to your commit transactions. $ git config --global color.ui auto - Enables helpuful colorization of command line output. BRANCHES All the commits will be made to the branches you are currently checked out to.\n1 2 3 4 5 6 $ git branch [branch-name] - Will create a new branch. $ git checkout [branch-name] - Switches to the specified branch and updates the working directory. $ git checkout [branch-name] -b - Creates a new branch and switches to the specified branch and updates the working directory. $ git merge [branch-name] - Combines the specified branch’s history into the current branch. This is usually done in pull requests. $ git branch -d [branch-name] - Deletes the specified branch. SYNCHRONIZE CHANGES Synchronize your local repository with the remote repository on GitHub.\n1 2 3 4 $ git push - Uploads all local branch commits to GitHub. $ git merge - Combines remote tracking branch into current local branch. $ git fetch - Downloads all history from the remote tracking branches. $ git pull - Updates your current local working branch with all new commits from the corresponding remote branch on GitHub. git pull is a combination of git fetch and git merge. MAKE CHANGES Browse and inspect the evolution of project files.\n1 2 3 $ git diff [first-branch]...[second-branch] - Shows content differences between two branches. $ git log --follow [file] - Lists version history for a file, including renames. $ git log - Lists version history for the current branch. REDO COMMITS Erase mistakes and craft replacement history.\n1 2 $ git reset [commit] - Undoes all commits after [commit], preserving changes locally. $ git reset --hard [commit] - Discards all history and changes back to the specified commit. ","date":"2022-11-11T13:23:31+05:30","permalink":"https://jhaakansha.github.io/p/git-cheatsheet/","title":"Git Cheatsheet"},{"content":"A stack is an ordered list in which insertion and deletion are done at one end called the \u0026ldquo;top\u0026rdquo;.\nMain Stack Operations Push: Inserts data onto the stack. Pull: Removes and returns the last inserted data from the stack. Auxiliary Stack Operations Top: Returns the last inserted element without removing it. Size: returns the number of elements stored in the stack. IsEmptyStack: Check whether the stack is empty or not. IsFullStack: Check whether the stack is full or not. Simple Array Implementation This implementation of stack uses simple array. We add elements from left to right and use a variable to keep track of the index of the top element. If the array i full, a push operation will throw a full stack exception. Similarly, if we try to delete an element from an empty array, it will throw a stack empty exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 struct StackArr { int top; int capacity; int *array; }; struct StackArr *CreateStack() { struct StackArr *S = malloc(sizeof(struct StackArr)); if (!S) { return NULL; } S-\u0026gt;capacity = 1; S-\u0026gt;top = -1; S-\u0026gt;array = malloc(S-\u0026gt;capacity*sizeof(int)); if(!S-\u0026gt;array) { return NULL; } return S; } int IsEmptyStack (struct StackArr *S) { return (S-\u0026gt;top == -1); } int IsFullStack (struct StackArr *S) { return (S-\u0026gt;top == S-\u0026gt;capacity - 1); } void Push (struct StackArr *S, int data) { if (IsFullStack(S)) { printf(\u0026#34;Stack Overflow\u0026#34;); } else { S-\u0026gt;top++; S-\u0026gt;array[S-\u0026gt;top]=data; } } int Pop (struct StackArr *S) { if (IsEMptyStack(S)) { printf(\u0026#34;Stack is empty\u0026#34;); return 0; } else { return (S-\u0026gt;array[top--]); } } Performance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) However, the maximum size of the array must be defined at the beginning and cannot be changed. Trying to push a new element into a full stack causes an implementation-specific exception. Therefore, a simple array implementation is not ideal and hence dynamic array implementation is preferred.\nDynamic Array Implementation In this approach, if the array is full, we create a new array of twice the size and copy items. With this approach, pushing n items takes time proportional to n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 struct DynArr { int top; int capacity; int *array; }; struct DynArr *CreateStack() { struct DynArr *S = malloc(sizeof(struct DynArr)); if(!S) { return NULL; } else { S-\u0026gt;capacity = 1; S-\u0026gt;top = -1; S-\u0026gt;array = malloc(S-\u0026gt;capacity*sizeof(int)); } if (S-\u0026gt;array) { return NULL; } return S; } int IsFullStack(struct DynArr *S) { return (S-\u0026gt;top == S-\u0026gt;capacity-1); } void DoubleStack(struct DynArr *S) { S-\u0026gt;capacity*=2; S-\u0026gt;array = realloc(S-\u0026gt;array, S-\u0026gt;capacity); } void Push(struct DynArr *S, int x) { if (IsFullStack(S)) { DoubleStack(S); } S-\u0026gt;array[++S-\u0026gt;top] = x; } int IsEmptyStack(struct DynArr *S) { return S-\u0026gt;top == -1; } int Top(struct DynArr *S) { if (IsEmptySrack(S)) { return INT_MIN; } return S-\u0026gt;array[S-\u0026gt;top]; } int Pop(struct DynArr *S) { if(IsEmptyStack(S)) { return INT_MIN; } return S-\u0026gt;array[S-\u0026gt;top--]; } Perfromance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) (Average) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) The limitation of this implementation of stack is that too many doublings may cause memory overflow exception.\nLinked List Implementation Another way of implementing stacks is by using linked lists. Push operation is implemented by inserting the incoming element at the beginning of the list. Pop operation is implemented by deleting the node from the beginning.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 struct ListNode { int data; struct ListNode *next; }; struct Stack *CreateStack() { return NULL; } void Push(struct Stack **top, int data) { struct Stack *temp; temp = malloc(sizeof(struct Stack)); if (!temp) { return NULL; } temp-\u0026gt;data = data; temp-\u0026gt;next = *top; *top = temp; } int IsEmptyStack(struct Stack *top) { return top == NULL; } int Pop(struct Stack **top) { int data; struct Stack *temp; if (IsEmptyStack(top)) { return INT_MIN; } temp = *top; *top = *top-\u0026gt;next; data = temp-\u0026gt;data; free(temp); return data; } int Top(struct Stack *top) { if (IsEmptyStack(top)) { return INT_MIN; } return top-\u0026gt;next-\u0026gt;data; } Performance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) (Average) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) Comparing Array Implementation and Stack Implementation Array Implementation\nOperations take constant time. Expensive doubling operations every once in a while. Any sequence of n operations takes time proportional to n. Linked List Implementation\nGrows and shrinks gracefully. Every operation takes constant time O(1). Every operation uses extra time and space to deal with references. ","date":"2022-11-05T16:23:59+05:30","permalink":"https://jhaakansha.github.io/p/stack/","title":"Stack"},{"content":"A linked list is a list or chain of items where each item points to the next one in the list. Each item in a linked list is called a node. Each node contains the data and the location of the next item.\nPROPERTIES Successive elements are connected by pointers. Last element points to NULL. Can grow or shrink in size during execution of a program. Can be made just as long as required. It does not waste memory space (but takes some extra memory for pointers). Disadvantages Large access time to individual element. An advantage of arrays in access time is special locality in memory. Arrays are defined as contiguous blocks of memory, and so any array element will be physically near its neighbours. This greatly benefits from modern CPU caching methods. Linked lists can be hard to manipulate. Waste memory in terms of extra reference points. SINGLY LINKED LIST Type declaration 1 2 3 4 struct listNode { int data; struct listNode *next; } Traversing the list Time: O(n) Space: O(1)\n1 2 3 4 5 6 7 8 9 int listLength (struct listNode *head) { struct listNode *current = head; int count= 0; while (currnet != NULL) { count++; current = current-\u0026gt;next; } return count; } Inserting an element Time: O(n) Space: o(1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 void insert(struct listNode **head, int data, int position) { int k = 1; struct listNode *p, *q, *newNode; newNode = (listNode*)malloc(sizeof(struct listNode)); if (!newNode) { printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; p = *head; if (position == 1) { newNode-\u0026gt;next = p; *head = newNode; } else { while ((p != NULL) \u0026amp;\u0026amp; (k \u0026lt; position - 1)) { k++; q = p; p = p-\u0026gt;next; } if (p == NULL) { q-\u0026gt;next = newNode; newNode-\u0026gt;next = NULL; } else { q-\u0026gt;next = newNode; newNode-\u0026gt;next = p; } } } Deleting a node 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 void deleteNode (struct listNode **head, int position) { int k = 1; struct listNode *p, *q; if (*head == NULL) { printf(\u0026#34;List Empty\u0026#34;); return; } p = *head; if (position == 1) { p = *head; *head = *head-\u0026gt;next; free(p); return; } else { while ((p != NULL) \u0026amp;\u0026amp; (k \u0026lt; position - 1)) { k++; q = p; p = p-\u0026gt;next; } if (p == NULL) { printf(\u0026#34;Position does not exist\u0026#34;); } else { q-\u0026gt;next = p-\u0026gt;next; free(p); } } } DOUBLY LINKED LIST In doubly linked lists, given anode, we can navigate the list in both directions.\nA node in a singly linked list cannot be removed unless we have the pointer to its predecessor. But in a doubly linked list, we can delete a node if we don\u0026rsquo;t have previous nodes, address (since each node has left pointer pointing to a previous node and we can move backwards).\nDisadvantages Each node requires an extra pointer. requiring more space. The insertion or deletion of a node takes a little longer. Type declaration 1 2 3 4 5 struct DLLnode { int data; struct DLLnode *next; struct DLLnode *prev; } Inserting an element 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 void DLLInsert(struct DLLnode **head, int data, int position) { int k = 1; struct DLLnode *temp, *newNode; newNode = (struct DLLnode*)malloc(sizeof(struct DLLnode)); if (!newNode) { printf(\u0026#34;Memory error\u0026#34;); return; } newNode-\u0026gt;data = data; if (position == 1) { //insert at the beginning newNode-\u0026gt;next = *head; newNode-\u0026gt;prev = NULL; *head-\u0026gt;prev = newNode; *head = newNode; return; } temp = *head; while( (k \u0026lt; position - 1) \u0026amp;\u0026amp; temp-\u0026gt;next != NULL) { temp = temp-\u0026gt;next; k++; } if (temp-\u0026gt;next == NULL) { //insert at the end newNode-\u0026gt;next = temp-\u0026gt;next; newNode-\u0026gt;prev = temp; temp-\u0026gt;next = newNode; } else { //in the middle newNode-\u0026gt;next = temp-\u0026gt;next; newNode-\u0026gt;prev = temp; temp-\u0026gt;next-\u0026gt;prev = newNode; temp-\u0026gt;next = newNode; } return; } Deleting a node 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 void DLLNode(struct DLLNode **head, int position) { struct DLLNode *temp, *temp2, temp = *head; int k = 1; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } if (position == 1) { //at the beginning *head = *head-\u0026gt;next; if (*head != NULL) { *head-\u0026gt;prev = NULL; } free(temp); return; } while ( (k\u0026lt;position - 1) \u0026amp;\u0026amp; temp-\u0026gt;next != NULL) { temp = temp-\u0026gt;next; k++; } if (temp-\u0026gt;next == NULL) { //from the end temp2 = temp-\u0026gt;prev; temp2-\u0026gt;next = NULL; free(temp); } else { //in the middle temp2 = temp-\u0026gt;prev; temp2-\u0026gt;next = temp-\u0026gt;next; temp-\u0026gt;next-\u0026gt;prev = temp2; free(temp); } return; } CIRCULAR LINKED LIST Type declaration 1 2 3 4 struct CLLnode { int data; struct CLLnode *next; } Inserting a node at the end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 void insertAtEnd(struct CLLnode **head, int data) { struct CLLnode current = *head; struct CLLnode *newNode = (struct node*)malloc(sizeof(struct CLLnode)); if (!newNode){ printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; while(current-\u0026gt;next != *head) { current = cuurent-\u0026gt;next; } newNode-\u0026gt;next = newNode; if (*head == NULL) { *head = newNode; } else { newNode-\u0026gt;next = *head; current-\u0026gt;next = newNode; } } Inserting a node at the front 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 void insertAtBegin(struct CLLnode **head, int data) { struct CLLnode *current = *head; struct CLLnode *newNode = (struct node*)malloc(sizeof(struct CLLnode)); if (!newNode) { printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; while (current-\u0026gt;next != *head) { current = current-\u0026gt;next; } newNode-\u0026gt;next = newNode; if (*head == NULL) { *head = newNode; } else { newNode-\u0026gt;next = *head; current-\u0026gt;next = newNode; *head = newNode; } return; } Deleting a node at the front 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void deleteFront(struct CLLnode **head) { struct CLLnode *temp = *head; struct CLLnode *current = *head; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } while (current-\u0026gt;next != *head) { current = current-\u0026gt;next; } current-\u0026gt;next = *head-\u0026gt;next; *head = *head-\u0026gt;next; free(temp); return; } Deleting a node from the end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 void deleteLast (struct CLLnode **head) { struct CLLNode *temp = *head; struct CLLnode *current = *head; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } while (current-\u0026gt;next != *head) { temp = current; current = current-\u0026gt;next; } free(current); return; } ","date":"2022-10-21T19:17:59+05:30","permalink":"https://jhaakansha.github.io/p/linked-list/","title":"Linked List"},{"content":"Runtime : O(log n)\nThe approximate middle item of the data set is located, and its key value is examined. If its value is too high, then the key of the middle element of the first half of the set is examined and procedure is repeated on the first half until the required item is found. If the value is too low, then the key of the middle entry of the second half of the data set is tried and the procedure is repeated on the second half. The process is continued until the desired key is found or search interval becomes empty.\nImplementation of Binary search Template 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int binarySearch (int a[], int n, int key) { int found = 0; mid, low = 0, high = n-1; while (low \u0026lt;= high) { mid = (low+high)/2; if (key \u0026lt; a[mid]) { high = mid - 1; } else if (key \u0026gt; a[mid]) { low = mid + 1; } else { found = 1; break; } } return found; } Template 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int binary(int[] arr, int key) { int low = 0, high = arr.length() - 1; while (low \u0026lt; high) { int mid = (high + low)/2; if (arr[mid] == key) { return mid; } else if (arr[mid] \u0026lt; key) { low = mid + 1; } else { high = mid; } } if (arr[low] == key) { return low; } return -1; } Template 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 int binary (int[] arr; int key) { int low = 0, high = arr.length() - 1; while (low + 1 \u0026lt; high) { int mid = (low + high)/2; if (arr[mid] == key) { return mid; } else if (arr[mid] \u0026lt; key) { low = mid; } else { high = mid; } } if (nums[low] == key) { return low; } if (nums[high] == key) { return high; } return -1; } RECURSIVE BINARY SEARCH Binary search can be implemented using recursion as well.\nImplementation of recursive Binary search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void binary(int arr[], int key, int low, int high) { int mid, flag = 0; if (low \u0026gt; high) { printf(\u0026#34;Element not found\u0026#34;); } mid = (low+high)/2; if (key \u0026lt; arr[mid]) { binary(arr, key, 0, mid - 1); } else if (key \u0026gt; arr[mid]) { binary(arr, key, mid+1, high); } else { flag = 1; } if (flag == 1) { printf(\u0026#34;Element found at index %d\u0026#34;, mid); } } ","date":"2022-10-07T04:35:32+05:30","permalink":"https://jhaakansha.github.io/p/binary-search/","title":"Binary Search"},{"content":"In this searching method, first of all, an index file is created that contains some specific group or division of required record when the index is obtained, then the partial indexing takes less time because it is located in a specific group.\nCharacteristics of indexed sequential search In indexed sequential search, a sorted index is set aside in addition to the array. Each element in the index points to a block of elements in the array or another expanded index. The index is searched first then the array and guides the search in the array. Implementation of Indexed Sequential Search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 int gn = 3; //gn is the number of elements in a group int elements[gn], indices[gn], i, set =0; int j = 0, ind = 0, start, end; for (i = 0; i \u0026lt; n; i+=3) { elements[ind] = arr[i]; indices[ind] = i; ind++; } if (k \u0026lt; elements[0]) { printf(\u0026#34;Not found\u0026#34;); exit(0); } else { for (i = 1; i \u0026lt;= ind; i++) { if (k \u0026lt;= elements[i]) { start = indices[i-1]; end = indices[i]; set = 1; break; } } if (set == 0) { start = indices[gn - 1]; end = gn; } for (i = start; i \u0026lt;= end; i++) { if (k == arr[i]) { j = 1; break; } } } if (j == 1) { printf(\u0026#34;Found at index %d\u0026#34;, i); } else { printf(\u0026#34;element not found\u0026#34;); } ","date":"2022-10-02T04:55:43+05:30","permalink":"https://jhaakansha.github.io/p/indexed-sequential-search/","title":"Indexed Sequential Search"},{"content":"Runtime: O(nlogn) Memory : O(1)\nHeap Sort is a comparison-based sorting technique based on binary heap data structure. It is an in-place algorithm. Its typical implementation is not stable, but can be made stable. Typically, it is 2-3 times slower than well-implemented quicksort due to lack of locality of reference.\nAdvantages of heapsort Efficiency : The time required to perform heap sort increases logarithmicallywhile other algorithms may grow exponentially slower as the number of items to sort increases. Memory usage : Memory usage is minimal because apart from what is necessary to hold the initial list of items to be sorted,it needs no additional memory space to work. Simplicity : It is simpler to understand than other sorting algorithm because it does not use advanced computer science concepts. Heapify It is the process of creating a heap data structure from a binary tree represented using an array. It is used to create Min-Heap or Max-Heap.Start from the first index of the non-leaf node whose index is given by n/2 - 1. Heapify using recursion.\nImplementation of Heap Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 void heapify (int arr[], int n, int i) { int largest = i; int left = 2*i + 1; int right = 2*i + 2; if (left \u0026lt; n \u0026amp;\u0026amp; arr[left] \u0026gt; arr[largest]){ largest = left; } if (right \u0026lt; n \u0026amp;\u0026amp; arr[right] \u0026gt; arr[largest]) { largest = right; } if (largest != i) { swap(\u0026amp;arr[i],\u0026amp;arr[largest]); heapify(arr, n, largest); } } void heapSort(int arr[], int n) { for (int i = n/2 - 1; i \u0026gt;= 0; i--) { heapify(arr, n, i); } for (int i = n - 1; i \u0026gt;= 0; i--) { swap(\u0026amp;arr[0], \u0026amp;arr[i]); heapify(arr, i, 0); } } What are the two phases of heap sort? Array is converted into max heap. Highest element is removed and the remaining elements are used to create a new max heap. Which is better: Heapsort or Mergesort? Mergesort is slightly faster than heapsort but mergesort requires extra storage space. Depending on the requirement, one should be chosen. Why is heapsort better than selection sort?\nHeapsort is similar to selection sort, but with a better way to get the maximum element. It takes advantage os the heap data structure to get the maximum element in constant time. ","date":"2022-09-11T10:44:35+05:30","permalink":"https://jhaakansha.github.io/p/heap-sort/","title":"Heap Sort"},{"content":"Runtime: average : O(nlogn) worst : O(n²) Auxiliary Space : O(n) In quick sort, we pick a random element and partition the array, such that all numbers that are less than the partitioning element come before all elements greater than it.\nIf we repeatedly partition the array around an element, the array will eventually become sorted. However, as the partitioned element is not huaranteed to be the median, our sorting could be very slow.\nQuick Sort is not a stable algorithm. However, any sorting algorithm can be made stable by considering indexes as comparison parametres.\nImplementation of QuickSort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 void quickSort(int[] arr, int left, int right) { int index = partition(arr, left, right); if (left\u0026lt;index-1) { quickSort(arr, left, index-1); } if (index \u0026lt; right) { quickSort(arr, index, right); } } int partition(int[] arr, int left, int right) { int pivot = arr[(left + right)/2]; while (left \u0026lt;= right) { while (arr[left] \u0026lt; pivot) { left++; } while (arr[right] \u0026gt; pivot) { right--; } if (left \u0026lt;= right) { swap(arr[left], arr[right]); left++; right--; } } return left; } Worst Case The worst case occurs when the partition process always picks the greatest or the smallest element as the pivot. If we consider the partition startegy where the last element is always picked as a pivot, the worst case obviously occurs when the array is already sorted.\nBest Case The best case occurs when the partition process always picks the middle element as the pivot.\n3 Way QuickSort Consider an array which has many redundant elements. For example {1,2,3,6,6,8,9,9,0,0,0,7}, If we pick 6 as the pivot, we fix only one 6 and recursively process remaining occurrences. The idea of 3 way quicksort is to process all occurrences of the pivot and is based on Dutch National Flag algorithm.\nImplementation of 3 way quicksort using Dutch National Flag algorithm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def partition (arr, first, last, start, mid): pivot = arr[last] end = last while (mid[0] \u0026lt;= end): if (arr[mid[0]] \u0026lt; pivot): arr[mid[0]], arr[start[0]] = arr[start[0]], arr[mid[0]] mid[0] = mid[0] + 1 start[0] = start[0] + 1 elif (arr[mid[0]] \u0026gt; pivot): arr[mid[0]], arr[end] = arr[end], arr[mid[0]] end = end - 1 else: mid[0] = mid[0] + 1 def quickSort(arr, first, last): if(first \u0026gt;= last): return if (last == first+1): if(arr[first] \u0026gt; arr[last]): arr[first], arr[last] = arr[last], arr[first] return start = [first] mid = [first] partition(arr, first, last, start, mid) quickSort(arr, first, start[0]-1) quickSort(arr, mid[0], last) The time complexity of this algorithm is O(nlogn) and the space complexity is o(logn).\nWhy is Quick Sort preferred over Merge Sort for sorting arrays? QuickSort is an in-place sort (i.e. it does not require any extra storage) whereas mergesort requires O(n) extra storage. Allocating and de-allocating the extra space used for mergesort increases the running time of the algorithm. Most practical implementations of QuickSort use randomized version. It has expected time complexity of O(nlogn). The worst case is possible in randomized version also, but worst case does not occur for a particular pattern (like sorted array) and works well in practice. QuickSort is also cache friendly sorting algorithm as it has good locality of reference when used for arrays. It is also tail recursive, therefore tail call optimization is done. ","date":"2022-09-03T13:18:51+05:30","permalink":"https://jhaakansha.github.io/p/quick-sort/","title":"Quick Sort"},{"content":"Runtime: O(nlogn) Auxiliary Space : O(n)\nMerge Sort divides the array in half, sorts each of those halves and then merges them back together. Each of these halves has the same sorting algorithm applied to it. Eventually, its like merging two single-element arrays. The merge method operates by copying all the elements from target array segment into a helper array,keeping track of where the start of the left and right halves should be. Then, iterate through helper, copying the smaller element from each half into the array.\nIt is a stable algorithm.\nImplementation of MergeSort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 void mergesort(int[] array) { int[] helper = new int[array.length]; mergesort(array, helper, 0, array.length - 1); } void mergesort(int[] array, int[] helper, int low, int high) { if (low \u0026lt; high) { int middle = (low+high)/2; mergesort(array, helper, low, middle); //sort left half mergesort(array, helper, middle+1, high); //sort right half mergesort(array, helper, low, middle, high); //merge them } } void merge(int[] array, int[] helper, int low, int middle, int high) { //copy both halves into a helper array for (int i = low; i\u0026lt;=high; i++) { helper[i] = array[i]; } int helperLeft = low; int helperRight = middle + 1; int current = low; while (helperLeft \u0026lt;= middle \u0026amp;\u0026amp; helperRight \u0026lt;= high) { if (helper[helperLeft] \u0026lt;= helper[helperRight]) { array[current] = helper[helperLeft]; helperLeft++; } else { array[current] = helper[helperRight]; helperRight++; } current++; } int remaining = middle - helperLeft; for (int = 0; i \u0026lt;= remaining; i++) { array[current + i] = helper[helperLeft + i]; } } Drawbacks It is slower compared to other sort algorithms for smaller tasks. It requires additional memory space of O(n). Merge sort goes through whole process even if array is sorted. ","date":"2022-08-26T13:18:43+05:30","permalink":"https://jhaakansha.github.io/p/merge-sort/","title":"Merge Sort"},{"content":"Best Case Runtime: O(n) Worst Case Runtime : O(n²)\nThe method for implementation of insertion sort is similar to the one we use to arrange our cards. It is a stable sorting algorithm. The steps followed are:\nIterate from arr[1] to arr[n] over the array. Compare the current element (key) to its predecessor. If the key element is smaller than its predecessor, compare it to the element before. Move the greater elements one position up to make space for the swapped element. Implementation of InsertionSort 1 2 3 4 5 6 7 8 9 for (i = 0; i \u0026lt; n; i++) { key = arr[i]; j = i - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; arr[j] \u0026gt; key) { arr[j+1] = arr[j]; j-=1; } key = arr[j]; } Binary Insertion Sort This method uses binary search to find the proper location to insert the selected item at each iteration. In normal insertion, sorting takes O(i) time (at ith iteration) in worst case, this can be reduced to O(log i). The algorithm, as a whole, still has the worst case running time of O(n²).\nImplementation of binary Insertion Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 int binarySearch(int a[], int item, int low, int high) { if (high \u0026lt;= low) { return ((item \u0026gt; a[low]) ? (low+1) : low) } int mid = (low+high)/2; if (item == a[mid]) { return mid+1; } if (item \u0026gt; a[mid]) { return (binarySearch(a, item, mid+1, high)); } } void insertionSort(int a[], int n) { int i, loc, j, k, selected; for (i = 1; i \u0026lt; n; ++i) { j = i-1; selected = a[i]; loc = binarySearch(a, selected, 0, j); while (j \u0026gt;= loc) { a[j+1] = a[j]; j--; } a[j+1] = selected; } } ","date":"2022-08-21T02:03:01+05:30","permalink":"https://jhaakansha.github.io/p/insertion-sort/","title":"Insertion Sort"},{"content":"Runtime: O(n²) Memory : O(1)\nFind the smallest element using a linear scan and move it to the front. Using linear scane, traverse the array from the second element onwards and find the least element in this sub-array. Swap the second element with the least element in the sub-array (which will the second least element in the array). Continue doing this until all elements are in place.\nThe default implementation of selection sort is not stable. Implementation of SelectionSort 1 2 3 4 5 6 7 8 9 10 11 for (i = 0; i \u0026lt; n - 1; i++) { min_idx = i; for (j = i+1; j \u0026lt; n; j++) { if (arr[j] \u0026lt; arr[min_idx]){ min_idx = j; } } if (min_idx != i) { swap(\u0026amp;arr[i], \u0026amp;arr[min_idx]); } } ","date":"2022-08-06T14:20:15+05:30","permalink":"https://jhaakansha.github.io/p/selection-sort/","title":"Selection Sort"},{"content":"Runtime: O(n²) Memory : O(1)\n*In this article each n refers to the number of elements present in the array.\nIn Bubble sort, we start at the beginning of the array and swap the first two elements if the first element is greater than the second. Then, we go to the next pair and so on, continuously making sweeps of the array until it is sorted.\nIt is a stable algorithm, i.e, the relative positions of equivalent elements remains the same.\nSimple BubbleSort 1 2 3 4 5 6 7 for (i = 0; i \u0026lt; n-1; i++) { for (j = 0; j \u0026lt; n-1; j++) { if (arr[j] \u0026gt; arr[j+1]) { swap(\u0026amp;arr[j], \u0026amp;arr[j+1]); } } } The above method is not optimized it could be bettered by stopping the algorithm if the inner loop does not cause any swap.\nThe optimized approach will run a little slower than the original one if all the passes are made. However, in the best situation, the time complexity will be O(n) as opposed to the original which was O(n²) in all circumstances.\nWorst Case : The worst case occurs when all the elements are arranged in descending order.\nTotal number of iterations = n - 1\nAt pass 1:\ncomparisons = n - 1 swaps = n - 1\nAt pass 2:\ncompartisons = n - 2 swaps = n - 2\nand so on until the number of comparisons is 1. Therefore, total number of comparisons required to sort the array\n= (n - 1) + (n - 2) + (n - 3) + \u0026hellip;. + 2 + 1\n= (n - 1)(n - 1 + 1)/2\n= n(n - 1)/2\nThis is equivalent to n².\nBest Case : The best occurs when the array is already sorted. The time complexity in this case is O(n).\nOptimized BubbleSort 1 2 3 4 5 6 7 8 9 10 11 12 for (i = 0; i \u0026lt; n-1; i++) { swapped = false; for (j = 0; j \u0026lt; n-i-1; j++) { if (arr[j] \u0026gt; arr[j+1]) { swap(\u0026amp;arr[j], \u0026amp;arr[j+1]); swapped = true; } } if (swapped == false) { break; } } Recursive BubbleSort The following steps are followed to implement recursive bubblesort:-\nPlace the largest element at its position, this operation makes sure that the first largest element will be placed at the end of the array. Recursively call for the rest n - 1 elements with the same operation and place the next greater element at its position. The base condition for this recursion call would be when the number of elements in the array becomes 0 or 1 then, simply return. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void recursiveBubble (int arr[], int len) { int temp, i; if (len == 0) { return; } for (i = 0; i \u0026lt; len-1; i++) { if (arr[i] \u0026gt; arr[i+1]) { temp = arr[i]; arr[i] = arr[i+1]; arr[i+1] = temp; } } len = len-1; recursiveBubble(arr,len); } ","date":"2022-07-30T12:27:42+05:30","permalink":"https://jhaakansha.github.io/p/bubble-sort/","title":"Bubble Sort"},{"content":"Databending \u0026amp; Glitch Art: Turning Digital Errors into Art Ever seen a photo glitch out—colors going wild, pixels melting into one another—and thought it actually looked kind of… cool? That’s basically what databending is all about. It’s the art of breaking digital stuff on purpose to make something unexpected and beautiful.\nThink of it like this: if circuit bending is the act of rewiring a kid’s toy or old keyboard to make weird sounds, databending is the digital version—only instead of wires, we’re messing with raw data.\nWhat Even Is Glitch Art? Glitch art is exactly what it sounds like: art made from glitches. Sometimes they happen by accident (your computer crashes and corrupts an image), and sometimes they’re 100% intentional. Artists have started embracing these weird little moments when tech fails—because in those moments, something raw and chaotic shows up. And it’s kind of beautiful.\nThere\u0026rsquo;s even some debate: does it have to be accidental to count as glitch art? Or can you plan a glitch? Either way, databending sits right in the middle—intentional chaos, but with roots in real, spontaneous errors.\nTools of the Trade To start databending, you don’t need fancy software. You just need the courage to open a file in the wrong program and see what happens.\nSome go-to tools:\nHex editors like HxD or Hex Fiend let you mess with the literal ones and zeroes inside a file. Audacity (yes, the audio editor) can open an image file as sound—which leads to some awesome, noisy results. Even a plain text editor like Notepad can break things in interesting ways. Types of Databending There are a few classic techniques people use:\n1. Incorrect Editing This is the OG move: open a file (like a JPEG) in a program meant for something totally different—like a sound editor—and just go wild. Save it, and open it again as an image. Boom, glitch.\n2. Reinterpretation Here, you\u0026rsquo;re converting files between formats in weird ways. Treating a picture as audio, or vice versa. Sometimes even just renaming the file extension can trip up your system in fun ways.\n3. Forced Errors This is where you exploit bugs on purpose. Maybe you know a certain plugin crashes under specific conditions—or that a program saves corrupt files if you interrupt it mid-process. So… you interrupt it on purpose. Glitchy magic.\nA Little Backstory Glitch art didn’t come out of nowhere. It’s got roots in hacker culture, experimental music, and early digital art scenes. Artists like Rosa Menkman have helped shape the conversation around glitch art, even writing whole books about the aesthetic of digital error (The Glitch Moment(um) is a must-read).\nYou might’ve seen databending in music videos (like Kanye’s “Welcome to Heartbreak”), or even in the recent surge of “datamoshing” in TikTok edits and motion graphics.\nWhy Break Something on Purpose? Because perfection gets boring.\nGlitches remind us that technology isn’t flawless. It’s messy, fragile, and sometimes totally unpredictable. That unpredictability can actually be freeing for artists—it gives them a way to break out of the usual patterns and create something raw and unique.\nIn a world where everything’s edited, filtered, and polished, glitch art feels like a breath of chaotic fresh air.\nWanna Try It? Here’s how to jump in:\nGrab a simple image file—like a .jpg or .bmp. Open it in a hex editor. But be careful—don’t mess with the first few lines (the header), or your file won’t open again. Make some small changes—swap numbers, delete a few bytes, paste in random stuff. Save it with a new name and open it in your image viewer. Bask in the glitch. Or go wild with Audacity:\nOpen a .jpg or .png as raw audio. Add distortion, reverse it, slow it down. Export it as raw data. Rename the file back to .jpg and open it again. Weird, right? Final Thoughts Databending is like digital vandalism—but in a good way. It’s about twisting tools, breaking files, and celebrating the unexpected results. Whether you’re into tech, art, or just the joy of messing with things you “shouldn’t,” databending offers a whole new way to express yourself.\nSo go ahead—break something.\nMore inspiration: Rosa Menkman | Glitch Artist Collective on Reddit\n","date":"2022-07-24T13:15:46+05:30","permalink":"https://jhaakansha.github.io/p/data-bending/","title":"Data Bending"},{"content":" Big O is the used to decribe the efficiency of algorithms. Academics use big O, big θ and big Ω to decribe runtimes. Big O Big Ω Big θ It describes upper bound on time It describes lower bound on time θ means both O and Ω An algorithm that requires O(n) time can also be described as requiring O(n²), O(n³), O(2ⁿ) etc. The algorithm is at least as fast as any of these If the runtime is decribed as Ω(n), then it can also be described by Ω(log n) and Ω(1). But, it won\u0026rsquo;t be faster than those runtimes That is, an algorithm is θ(n) if its both O(n) and Ω(n). θ gives a tight bound on runtimes In industry, the meaning of O and θ seem to have been merged. Industry\u0026rsquo;s meaning of big O is closer to what academics mean by θ. The runtime for an algorithm is described in three different ways:\nBest Case : If we consider the case of sorting an array, then if all the elements are equal, then quick sort will traverse the array only once giving the runtime O(n). This is the least runtime possible, hence it is the best case scenario. Worst Case : If we get really unlucky and the pivot is repeatedly the biggest element in the array then we have the largest possible runtime. This is the worst case scenario. Expected Case : Usually, the conditions considered above don\u0026rsquo;t happen. In general cases, the expected runtime for quick sort will be O(n log n). ","date":"2022-07-14T12:09:29+05:30","permalink":"https://jhaakansha.github.io/p/big-o-notation/","title":"Big O Notation"}]