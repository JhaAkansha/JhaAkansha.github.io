[{"content":"CREATE REPOSITORIES New repostiories can be created either locally or by copying a file that alredy exists on GitHub.\n1 2 $ git init - Turn an existing directory into git repository. $ git clone [url] - Clone a repository that already exists in GitHub including all the files, branches and commits. CONFIGURE Configure user information for all local repositories.\n1 2 3 $ git config --global user.name \u0026#34;[name]\u0026#34; - Sets the name you want attached to your commit transactions. $ git config --global user.email \u0026#34;[email-address]\u0026#34; - Sets the email you want attached to your commit transactions. $ git config --global color.ui auto - Enables helpuful colorization of command line output. BRANCHES All the commits will be made to the branches you are currently checked out to.\n1 2 3 4 5 6 $ git branch [branch-name] - Will create a new branch. $ git checkout [branch-name] - Switches to the specified branch and updates the working directory. $ git checkout [branch-name] -b - Creates a new branch and switches to the specified branch and updates the working directory. $ git merge [branch-name] - Combines the specified branch’s history into the current branch. This is usually done in pull requests. $ git branch -d [branch-name] - Deletes the specified branch. SYNCHRONIZE CHANGES Synchronize your local repository with the remote repository on GitHub.\n1 2 3 4 $ git push - Uploads all local branch commits to GitHub. $ git merge - Combines remote tracking branch into current local branch. $ git fetch - Downloads all history from the remote tracking branches. $ git pull - Updates your current local working branch with all new commits from the corresponding remote branch on GitHub. git pull is a combination of git fetch and git merge. MAKE CHANGES Browse and inspect the evolution of project files.\n1 2 3 $ git diff [first-branch]...[second-branch] - Shows content differences between two branches. $ git log --follow [file] - Lists version history for a file, including renames. $ git log - Lists version history for the current branch. REDO COMMITS Erase mistakes and craft replacement history.\n1 2 $ git reset [commit] - Undoes all commits after [commit], preserving changes locally. $ git reset --hard [commit] - Discards all history and changes back to the specified commit. ","date":"2022-10-14T13:23:31+05:30","permalink":"https://jhaakansha.github.io/p/git-cheatsheet/","title":"Git Cheatsheet"},{"content":"A stack is an ordered list in which insertion and deletion are done at one end called the \u0026ldquo;top\u0026rdquo;.\nMain Stack Operations Push: Inserts data onto the stack. Pull: Removes and returns the last inserted data from the stack. Auxiliary Stack Operations Top: Returns the last inserted element without removing it. Size: returns the number of elements stored in the stack. IsEmptyStack: Check whether the stack is empty or not. IsFullStack: Check whether the stack is full or not. Simple Array Implementation This implementation of stack uses simple array. We add elements from left to right and use a variable to keep track of the index of the top element. If the array i full, a push operation will throw a full stack exception. Similarly, if we try to delete an element from an empty array, it will throw a stack empty exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 struct StackArr { int top; int capacity; int *array; }; struct StackArr *CreateStack() { struct StackArr *S = malloc(sizeof(struct StackArr)); if (!S) { return NULL; } S-\u0026gt;capacity = 1; S-\u0026gt;top = -1; S-\u0026gt;array = malloc(S-\u0026gt;capacity*sizeof(int)); if(!S-\u0026gt;array) { return NULL; } return S; } int IsEmptyStack (struct StackArr *S) { return (S-\u0026gt;top == -1); } int IsFullStack (struct StackArr *S) { return (S-\u0026gt;top == S-\u0026gt;capacity - 1); } void Push (struct StackArr *S, int data) { if (IsFullStack(S)) { printf(\u0026#34;Stack Overflow\u0026#34;); } else { S-\u0026gt;top++; S-\u0026gt;array[S-\u0026gt;top]=data; } } int Pop (struct StackArr *S) { if (IsEMptyStack(S)) { printf(\u0026#34;Stack is empty\u0026#34;); return 0; } else { return (S-\u0026gt;array[top--]); } } Performance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) However, the maximum size of the array must be defined at the beginning and cannot be changed. Trying to push a new element into a full stack causes an implementation-specific exception. Therefore, a simple array implementation is not ideal and hence dynamic array implementation is preferred.\nDynamic Array Implementation In this approach, if the array is full, we create a new array of twice the size and copy items. With this approach, pushing n items takes time proportional to n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 struct DynArr { int top; int capacity; int *array; }; struct DynArr *CreateStack() { struct DynArr *S = malloc(sizeof(struct DynArr)); if(!S) { return NULL; } else { S-\u0026gt;capacity = 1; S-\u0026gt;top = -1; S-\u0026gt;array = malloc(S-\u0026gt;capacity*sizeof(int)); } if (S-\u0026gt;array) { return NULL; } return S; } int IsFullStack(struct DynArr *S) { return (S-\u0026gt;top == S-\u0026gt;capacity-1); } void DoubleStack(struct DynArr *S) { S-\u0026gt;capacity*=2; S-\u0026gt;array = realloc(S-\u0026gt;array, S-\u0026gt;capacity); } void Push(struct DynArr *S, int x) { if (IsFullStack(S)) { DoubleStack(S); } S-\u0026gt;array[++S-\u0026gt;top] = x; } int IsEmptyStack(struct DynArr *S) { return S-\u0026gt;top == -1; } int Top(struct DynArr *S) { if (IsEmptySrack(S)) { return INT_MIN; } return S-\u0026gt;array[S-\u0026gt;top]; } int Pop(struct DynArr *S) { if(IsEmptyStack(S)) { return INT_MIN; } return S-\u0026gt;array[S-\u0026gt;top--]; } Perfromance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) (Average) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) The limitation of this implementation of stack is that too many doublings may cause memory overflow exception.\nLinked List Implementation Another way of implementing stacks is by using linked lists. Push operation is implemented by inserting the incoming element at the beginning of the list. Pop operation is implemented by deleting the node from the beginning.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 struct ListNode { int data; struct ListNode *next; }; struct Stack *CreateStack() { return NULL; } void Push(struct Stack **top, int data) { struct Stack *temp; temp = malloc(sizeof(struct Stack)); if (!temp) { return NULL; } temp-\u0026gt;data = data; temp-\u0026gt;next = *top; *top = temp; } int IsEmptyStack(struct Stack *top) { return top == NULL; } int Pop(struct Stack **top) { int data; struct Stack *temp; if (IsEmptyStack(top)) { return INT_MIN; } temp = *top; *top = *top-\u0026gt;next; data = temp-\u0026gt;data; free(temp); return data; } int Top(struct Stack *top) { if (IsEmptyStack(top)) { return INT_MIN; } return top-\u0026gt;next-\u0026gt;data; } Performance Function Time complexity Space Complexity (for n push operations) O(n) Time Complexity of Push() O(1) (Average) Time Complexity of Pop() O(1) Time Complexity of Size() O(1) Time Complexity of IsEmptyStack() O(1) Time Complexity of IsFullStack() O(1) Comparing Array Implementation and Stack Implementation Array Implementation\nOperations take constant time. Expensive doubling operations every once in a while. Any sequence of n operations takes time proportional to n. Linked List Implementation\nGrows and shrinks gracefully. Every operation takes constant time O(1). Every operation uses extra time and space to deal with references. ","date":"2022-10-07T16:23:59+05:30","permalink":"https://jhaakansha.github.io/p/stack/","title":"Stack"},{"content":"A linked list is a list or chain of items where each item points to the next one in the list. Each item in a linked list is called a node. Each node contains the data and the location of the next item.\nPROPERTIES Successive elements are connected by pointers. Last element points to NULL. Can grow or shrink in size during execution of a program. Can be made just as long as required. It does not waste memory space (but takes some extra memory for pointers). Disadvantages Large access time to individual element. An advantage of arrays in access time is special locality in memory. Arrays are defined as contiguous blocks of memory, and so any array element will be physically near its neighbours. This greatly benefits from modern CPU caching methods. Linked lists can be hard to manipulate. Waste memory in terms of extra reference points. SINGLY LINKED LIST Type declaration 1 2 3 4 struct listNode { int data; struct listNode *next; } Traversing the list Time: O(n) Space: O(1)\n1 2 3 4 5 6 7 8 9 int listLength (struct listNode *head) { struct listNode *current = head; int count= 0; while (currnet != NULL) { count++; current = current-\u0026gt;next; } return count; } Inserting an element Time: O(n) Space: o(1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 void insert(struct listNode **head, int data, int position) { int k = 1; struct listNode *p, *q, *newNode; newNode = (listNode*)malloc(sizeof(struct listNode)); if (!newNode) { printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; p = *head; if (position == 1) { newNode-\u0026gt;next = p; *head = newNode; } else { while ((p != NULL) \u0026amp;\u0026amp; (k \u0026lt; position - 1)) { k++; q = p; p = p-\u0026gt;next; } if (p == NULL) { q-\u0026gt;next = newNode; newNode-\u0026gt;next = NULL; } else { q-\u0026gt;next = newNode; newNode-\u0026gt;next = p; } } } Deleting a node 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 void deleteNode (struct listNode **head, int position) { int k = 1; struct listNode *p, *q; if (*head == NULL) { printf(\u0026#34;List Empty\u0026#34;); return; } p = *head; if (position == 1) { p = *head; *head = *head-\u0026gt;next; free(p); return; } else { while ((p != NULL) \u0026amp;\u0026amp; (k \u0026lt; position - 1)) { k++; q = p; p = p-\u0026gt;next; } if (p == NULL) { printf(\u0026#34;Position does not exist\u0026#34;); } else { q-\u0026gt;next = p-\u0026gt;next; free(p); } } } DOUBLY LINKED LIST In doubly linked lists, given anode, we can navigate the list in both directions.\nA node in a singly linked list cannot be removed unless we have the pointer to its predecessor. But in a doubly linked list, we can delete a node if we don\u0026rsquo;t have previous nodes, address (since each node has left pointer pointing to a previous node and we can move backwards).\nDisadvantages Each node requires an extra pointer. requiring more space. The insertion or deletion of a node takes a little longer. Type declaration 1 2 3 4 5 struct DLLnode { int data; struct DLLnode *next; struct DLLnode *prev; } Inserting an element 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 void DLLInsert(struct DLLnode **head, int data, int position) { int k = 1; struct DLLnode *temp, *newNode; newNode = (struct DLLnode*)malloc(sizeof(struct DLLnode)); if (!newNode) { printf(\u0026#34;Memory error\u0026#34;); return; } newNode-\u0026gt;data = data; if (position == 1) { //insert at the beginning newNode-\u0026gt;next = *head; newNode-\u0026gt;prev = NULL; *head-\u0026gt;prev = newNode; *head = newNode; return; } temp = *head; while( (k \u0026lt; position - 1) \u0026amp;\u0026amp; temp-\u0026gt;next != NULL) { temp = temp-\u0026gt;next; k++; } if (temp-\u0026gt;next == NULL) { //insert at the end newNode-\u0026gt;next = temp-\u0026gt;next; newNode-\u0026gt;prev = temp; temp-\u0026gt;next = newNode; } else { //in the middle newNode-\u0026gt;next = temp-\u0026gt;next; newNode-\u0026gt;prev = temp; temp-\u0026gt;next-\u0026gt;prev = newNode; temp-\u0026gt;next = newNode; } return; } Deleting a node 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 void DLLNode(struct DLLNode **head, int position) { struct DLLNode *temp, *temp2, temp = *head; int k = 1; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } if (position == 1) { //at the beginning *head = *head-\u0026gt;next; if (*head != NULL) { *head-\u0026gt;prev = NULL; } free(temp); return; } while ( (k\u0026lt;position - 1) \u0026amp;\u0026amp; temp-\u0026gt;next != NULL) { temp = temp-\u0026gt;next; k++; } if (temp-\u0026gt;next == NULL) { //from the end temp2 = temp-\u0026gt;prev; temp2-\u0026gt;next = NULL; free(temp); } else { //in the middle temp2 = temp-\u0026gt;prev; temp2-\u0026gt;next = temp-\u0026gt;next; temp-\u0026gt;next-\u0026gt;prev = temp2; free(temp); } return; } CIRCULAR LINKED LIST Type declaration 1 2 3 4 struct CLLnode { int data; struct CLLnode *next; } Inserting a node at the end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 void insertAtEnd(struct CLLnode **head, int data) { struct CLLnode current = *head; struct CLLnode *newNode = (struct node*)malloc(sizeof(struct CLLnode)); if (!newNode){ printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; while(current-\u0026gt;next != *head) { current = cuurent-\u0026gt;next; } newNode-\u0026gt;next = newNode; if (*head == NULL) { *head = newNode; } else { newNode-\u0026gt;next = *head; current-\u0026gt;next = newNode; } } Inserting a node at the front 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 void insertAtBegin(struct CLLnode **head, int data) { struct CLLnode *current = *head; struct CLLnode *newNode = (struct node*)malloc(sizeof(struct CLLnode)); if (!newNode) { printf(\u0026#34;Memory Error\u0026#34;); return; } newNode-\u0026gt;data = data; while (current-\u0026gt;next != *head) { current = current-\u0026gt;next; } newNode-\u0026gt;next = newNode; if (*head == NULL) { *head = newNode; } else { newNode-\u0026gt;next = *head; current-\u0026gt;next = newNode; *head = newNode; } return; } Deleting a node at the front 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void deleteFront(struct CLLnode **head) { struct CLLnode *temp = *head; struct CLLnode *current = *head; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } while (current-\u0026gt;next != *head) { current = current-\u0026gt;next; } current-\u0026gt;next = *head-\u0026gt;next; *head = *head-\u0026gt;next; free(temp); return; } Deleting a node from the end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 void deleteLast (struct CLLnode **head) { struct CLLNode *temp = *head; struct CLLnode *current = *head; if (*head == NULL) { printf(\u0026#34;List is empty\u0026#34;); return; } while (current-\u0026gt;next != *head) { temp = current; current = current-\u0026gt;next; } free(current); return; } ","date":"2022-10-02T19:17:59+05:30","permalink":"https://jhaakansha.github.io/p/linked-list/","title":"Linked List"},{"content":"Runtime : O(log n)\nThe approximate middle item of the data set is located, and its key value is examined. If its value is too high, then the key of the middle element of the first half of the set is examined and procedure is repeated on the first half until the required item is found. If the value is too low, then the key of the middle entry of the second half of the data set is tried and the procedure is repeated on the second half. The process is continued until the desired key is found or search interval becomes empty.\nImplementation of Binary search Template 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int binarySearch (int a[], int n, int key) { int found = 0; mid, low = 0, high = n-1; while (low \u0026lt;= high) { mid = (low+high)/2; if (key \u0026lt; a[mid]) { high = mid - 1; } else if (key \u0026gt; a[mid]) { low = mid + 1; } else { found = 1; break; } } return found; } Template 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 int binary(int[] arr, int key) { int low = 0, high = arr.length() - 1; while (low \u0026lt; high) { int mid = (high + low)/2; if (arr[mid] == key) { return mid; } else if (arr[mid] \u0026lt; key) { low = mid + 1; } else { high = mid; } } if (arr[low] == key) { return low; } return -1; } Template 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 int binary (int[] arr; int key) { int low = 0, high = arr.length() - 1; while (low + 1 \u0026lt; high) { int mid = (low + high)/2; if (arr[mid] == key) { return mid; } else if (arr[mid] \u0026lt; key) { low = mid; } else { high = mid; } } if (nums[low] == key) { return low; } if (nums[high] == key) { return high; } return -1; } RECURSIVE BINARY SEARCH Binary search can be implemented using recursion as well.\nImplementation of recursive Binary search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void binary(int arr[], int key, int low, int high) { int mid, flag = 0; if (low \u0026gt; high) { printf(\u0026#34;Element not found\u0026#34;); } mid = (low+high)/2; if (key \u0026lt; arr[mid]) { binary(arr, key, 0, mid - 1); } else if (key \u0026gt; arr[mid]) { binary(arr, key, mid+1, high); } else { flag = 1; } if (flag == 1) { printf(\u0026#34;Element found at index %d\u0026#34;, mid); } } ","date":"2022-09-21T04:35:32+05:30","permalink":"https://jhaakansha.github.io/p/binary-search/","title":"Binary Search"},{"content":"In this searching method, first of all, an index file is created that contains some specific group or division of required record when the index is obtained, then the partial indexing takes less time because it is located in a specific group.\nCharacteristics of indexed sequential search In indexed sequential search, a sorted index is set aside in addition to the array. Each element in the index points to a block of elements in the array or another expanded index. The index is searched first then the array and guides the search in the array. Implementation of Indexed Sequential Search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 int gn = 3; //gn is the number of elements in a group int elements[gn], indices[gn], i, set =0; int j = 0, ind = 0, start, end; for (i = 0; i \u0026lt; n; i+=3) { elements[ind] = arr[i]; indices[ind] = i; ind++; } if (k \u0026lt; elements[0]) { printf(\u0026#34;Not found\u0026#34;); exit(0); } else { for (i = 1; i \u0026lt;= ind; i++) { if (k \u0026lt;= elements[i]) { start = indices[i-1]; end = indices[i]; set = 1; break; } } if (set == 0) { start = indices[gn - 1]; end = gn; } for (i = start; i \u0026lt;= end; i++) { if (k == arr[i]) { j = 1; break; } } } if (j == 1) { printf(\u0026#34;Found at index %d\u0026#34;, i); } else { printf(\u0026#34;element not found\u0026#34;); } ","date":"2022-09-11T04:55:43+05:30","permalink":"https://jhaakansha.github.io/p/indexed-sequential-search/","title":"Indexed Sequential Search"},{"content":"Runtime: O(nlogn) Memory : O(1)\nHeap Sort is a comparison-based sorting technique based on binary heap data structure. It is an in-place algorithm. Its typical implementation is not stable, but can be made stable. Typically, it is 2-3 times slower than well-implemented quicksort due to lack of locality of reference.\nAdvantages of heapsort Efficiency : The time required to perform heap sort increases logarithmicallywhile other algorithms may grow exponentially slower as the number of items to sort increases. Memory usage : Memory usage is minimal because apart from what is necessary to hold the initial list of items to be sorted,it needs no additional memory space to work. Simplicity : It is simpler to understand than other sorting algorithm because it does not use advanced computer science concepts. Heapify It is the process of creating a heap data structure from a binary tree represented using an array. It is used to create Min-Heap or Max-Heap.Start from the first index of the non-leaf node whose index is given by n/2 - 1. Heapify using recursion.\nImplementation of Heap Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 void heapify (int arr[], int n, int i) { int largest = i; int left = 2*i + 1; int right = 2*i + 2; if (left \u0026lt; n \u0026amp;\u0026amp; arr[left] \u0026gt; arr[largest]){ largest = left; } if (right \u0026lt; n \u0026amp;\u0026amp; arr[right] \u0026gt; arr[largest]) { largest = right; } if (largest != i) { swap(\u0026amp;arr[i],\u0026amp;arr[largest]); heapify(arr, n, largest); } } void heapSort(int arr[], int n) { for (int i = n/2 - 1; i \u0026gt;= 0; i--) { heapify(arr, n, i); } for (int i = n - 1; i \u0026gt;= 0; i--) { swap(\u0026amp;arr[0], \u0026amp;arr[i]); heapify(arr, i, 0); } } What are the two phases of heap sort? Array is converted into max heap. Highest element is removed and the remaining elements are used to create a new max heap. Which is better: Heapsort or Mergesort? Mergesort is slightly faster than heapsort but mergesort requires extra storage space. Depending on the requirement, one should be chosen. Why is heapsort better than selection sort?\nHeapsort is similar to selection sort, but with a better way to get the maximum element. It takes advantage os the heap data structure to get the maximum element in constant time. ","date":"2022-09-03T10:44:35+05:30","permalink":"https://jhaakansha.github.io/p/heap-sort/","title":"Heap Sort"},{"content":"Runtime: average : O(nlogn) worst : O(n²) Auxiliary Space : O(n) In quick sort, we pick a random element and partition the array, such that all numbers that are less than the partitioning element come before all elements greater than it.\nIf we repeatedly partition the array around an element, the array will eventually become sorted. However, as the partitioned element is not huaranteed to be the median, our sorting could be very slow.\nQuick Sort is not a stable algorithm. However, any sorting algorithm can be made stable by considering indexes as comparison parametres.\nImplementation of QuickSort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 void quickSort(int[] arr, int left, int right) { int index = partition(arr, left, right); if (left\u0026lt;index-1) { quickSort(arr, left, index-1); } if (index \u0026lt; right) { quickSort(arr, index, right); } } int partition(int[] arr, int left, int right) { int pivot = arr[(left + right)/2]; while (left \u0026lt;= right) { while (arr[left] \u0026lt; pivot) { left++; } while (arr[right] \u0026gt; pivot) { right--; } if (left \u0026lt;= right) { swap(arr[left], arr[right]); left++; right--; } } return left; } Worst Case The worst case occurs when the partition process always picks the greatest or the smallest element as the pivot. If we consider the partition startegy where the last element is always picked as a pivot, the worst case obviously occurs when the array is already sorted.\nBest Case The best case occurs when the partition process always picks the middle element as the pivot.\n3 Way QuickSort Consider an array which has many redundant elements. For example {1,2,3,6,6,8,9,9,0,0,0,7}, If we pick 6 as the pivot, we fix only one 6 and recursively process remaining occurrences. The idea of 3 way quicksort is to process all occurrences of the pivot and is based on Dutch National Flag algorithm.\nImplementation of 3 way quicksort using Dutch National Flag algorithm\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def partition (arr, first, last, start, mid): pivot = arr[last] end = last while (mid[0] \u0026lt;= end): if (arr[mid[0]] \u0026lt; pivot): arr[mid[0]], arr[start[0]] = arr[start[0]], arr[mid[0]] mid[0] = mid[0] + 1 start[0] = start[0] + 1 elif (arr[mid[0]] \u0026gt; pivot): arr[mid[0]], arr[end] = arr[end], arr[mid[0]] end = end - 1 else: mid[0] = mid[0] + 1 def quickSort(arr, first, last): if(first \u0026gt;= last): return if (last == first+1): if(arr[first] \u0026gt; arr[last]): arr[first], arr[last] = arr[last], arr[first] return start = [first] mid = [first] partition(arr, first, last, start, mid) quickSort(arr, first, start[0]-1) quickSort(arr, mid[0], last) The time complexity of this algorithm is O(nlogn) and the space complexity is o(logn).\nWhy is Quick Sort preferred over Merge Sort for sorting arrays? QuickSort is an in-place sort (i.e. it does not require any extra storage) whereas mergesort requires O(n) extra storage. Allocating and de-allocating the extra space used for mergesort increases the running time of the algorithm. Most practical implementations of QuickSort use randomized version. It has expected time complexity of O(nlogn). The worst case is possible in randomized version also, but worst case does not occur for a particular pattern (like sorted array) and works well in practice. QuickSort is also cache friendly sorting algorithm as it has good locality of reference when used for arrays. It is also tail recursive, therefore tail call optimization is done. ","date":"2022-08-26T13:18:51+05:30","permalink":"https://jhaakansha.github.io/p/quick-sort/","title":"Quick Sort"},{"content":"Runtime: O(nlogn) Auxiliary Space : O(n)\nMerge Sort divides the array in half, sorts each of those halves and then merges them back together. Each of these halves has the same sorting algorithm applied to it. Eventually, its like merging two single-element arrays. The merge method operates by copying all the elements from target array segment into a helper array,keeping track of where the start of the left and right halves should be. Then, iterate through helper, copying the smaller element from each half into the array.\nIt is a stable algorithm.\nImplementation of MergeSort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 void mergesort(int[] array) { int[] helper = new int[array.length]; mergesort(array, helper, 0, array.length - 1); } void mergesort(int[] array, int[] helper, int low, int high) { if (low \u0026lt; high) { int middle = (low+high)/2; mergesort(array, helper, low, middle); //sort left half mergesort(array, helper, middle+1, high); //sort right half mergesort(array, helper, low, middle, high); //merge them } } void merge(int[] array, int[] helper, int low, int middle, int high) { //copy both halves into a helper array for (int i = low; i\u0026lt;=high; i++) { helper[i] = array[i]; } int helperLeft = low; int helperRight = middle + 1; int current = low; while (helperLeft \u0026lt;= middle \u0026amp;\u0026amp; helperRight \u0026lt;= high) { if (helper[helperLeft] \u0026lt;= helper[helperRight]) { array[current] = helper[helperLeft]; helperLeft++; } else { array[current] = helper[helperRight]; helperRight++; } current++; } int remaining = middle - helperLeft; for (int = 0; i \u0026lt;= remaining; i++) { array[current + i] = helper[helperLeft + i]; } } Drawbacks It is slower compared to other sort algorithms for smaller tasks. It requires additional memory space of O(n). Merge sort goes through whole process even if array is sorted. ","date":"2022-08-21T13:18:43+05:30","permalink":"https://jhaakansha.github.io/p/merge-sort/","title":"Merge Sort"},{"content":"Best Case Runtime: O(n) Worst Case Runtime : O(n²)\nThe method for implementation of insertion sort is similar to the one we use to arrange our cards. It is a stable sorting algorithm. The steps followed are:\nIterate from arr[1] to arr[n] over the array. Compare the current element (key) to its predecessor. If the key element is smaller than its predecessor, compare it to the element before. Move the greater elements one position up to make space for the swapped element. Implementation of InsertionSort 1 2 3 4 5 6 7 8 9 for (i = 0; i \u0026lt; n; i++) { key = arr[i]; j = i - 1; while (j \u0026gt;= 0 \u0026amp;\u0026amp; arr[j] \u0026gt; key) { arr[j+1] = arr[j]; j-=1; } key = arr[j]; } Binary Insertion Sort This method uses binary search to find the proper location to insert the selected item at each iteration. In normal insertion, sorting takes O(i) time (at ith iteration) in worst case, this can be reduced to O(log i). The algorithm, as a whole, still has the worst case running time of O(n²).\nImplementation of binary Insertion Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 int binarySearch(int a[], int item, int low, int high) { if (high \u0026lt;= low) { return ((item \u0026gt; a[low]) ? (low+1) : low) } int mid = (low+high)/2; if (item == a[mid]) { return mid+1; } if (item \u0026gt; a[mid]) { return (binarySearch(a, item, mid+1, high)); } } void insertionSort(int a[], int n) { int i, loc, j, k, selected; for (i = 1; i \u0026lt; n; ++i) { j = i-1; selected = a[i]; loc = binarySearch(a, selected, 0, j); while (j \u0026gt;= loc) { a[j+1] = a[j]; j--; } a[j+1] = selected; } } ","date":"2022-08-13T02:03:01+05:30","permalink":"https://jhaakansha.github.io/p/insertion-sort/","title":"Insertion Sort"},{"content":"Runtime: O(n²) Memory : O(1)\nFind the smallest element using a linear scan and move it to the front. Using linear scane, traverse the array from the second element onwards and find the least element in this sub-array. Swap the second element with the least element in the sub-array (which will the second least element in the array). Continue doing this until all elements are in place.\nThe default implementation of selection sort is not stable. Implementation of SelectionSort 1 2 3 4 5 6 7 8 9 10 11 for (i = 0; i \u0026lt; n - 1; i++) { min_idx = i; for (j = i+1; j \u0026lt; n; j++) { if (arr[j] \u0026lt; arr[min_idx]){ min_idx = j; } } if (min_idx != i) { swap(\u0026amp;arr[i], \u0026amp;arr[min_idx]); } } ","date":"2022-08-06T14:20:15+05:30","permalink":"https://jhaakansha.github.io/p/selection-sort/","title":"Selection Sort"},{"content":"Runtime: O(n²) Memory : O(1)\n*In this article each n refers to the number of elements present in the array.\nIn Bubble sort, we start at the beginning of the array and swap the first two elements if the first element is greater than the second. Then, we go to the next pair and so on, continuously making sweeps of the array until it is sorted.\nIt is a stable algorithm, i.e, the relative positions of equivalent elements remains the same.\nSimple BubbleSort 1 2 3 4 5 6 7 for (i = 0; i \u0026lt; n-1; i++) { for (j = 0; j \u0026lt; n-1; j++) { if (arr[j] \u0026gt; arr[j+1]) { swap(\u0026amp;arr[j], \u0026amp;arr[j+1]); } } } The above method is not optimized it could be bettered by stopping the algorithm if the inner loop does not cause any swap.\nThe optimized approach will run a little slower than the original one if all the passes are made. However, in the best situation, the time complexity will be O(n) as opposed to the original which was O(n²) in all circumstances.\nWorst Case : The worst case occurs when all the elements are arranged in descending order.\nTotal number of iterations = n - 1\nAt pass 1:\ncomparisons = n - 1 swaps = n - 1\nAt pass 2:\ncompartisons = n - 2 swaps = n - 2\nand so on until the number of comparisons is 1. Therefore, total number of comparisons required to sort the array\n= (n - 1) + (n - 2) + (n - 3) + \u0026hellip;. + 2 + 1\n= (n - 1)(n - 1 + 1)/2\n= n(n - 1)/2\nThis is equivalent to n².\nBest Case : The best occurs when the array is already sorted. The time complexity in this case is O(n).\nOptimized BubbleSort 1 2 3 4 5 6 7 8 9 10 11 12 for (i = 0; i \u0026lt; n-1; i++) { swapped = false; for (j = 0; j \u0026lt; n-i-1; j++) { if (arr[j] \u0026gt; arr[j+1]) { swap(\u0026amp;arr[j], \u0026amp;arr[j+1]); swapped = true; } } if (swapped == false) { break; } } Recursive BubbleSort The following steps are followed to implement recursive bubblesort:-\nPlace the largest element at its position, this operation makes sure that the first largest element will be placed at the end of the array. Recursively call for the rest n - 1 elements with the same operation and place the next greater element at its position. The base condition for this recursion call would be when the number of elements in the array becomes 0 or 1 then, simply return. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 void recursiveBubble (int arr[], int len) { int temp, i; if (len == 0) { return; } for (i = 0; i \u0026lt; len-1; i++) { if (arr[i] \u0026gt; arr[i+1]) { temp = arr[i]; arr[i] = arr[i+1]; arr[i+1] = temp; } } len = len-1; recursiveBubble(arr,len); } ","date":"2022-07-30T12:27:42+05:30","permalink":"https://jhaakansha.github.io/p/bubble-sort/","title":"Bubble Sort"},{"content":" Databending is the artistic misuse of digital information. It draws its name from the practice of circuit bending \u0026ndash; a practice where childrens\u0026rsquo; toys, cheap keyboards and effects pedals are deliberately short-circuited by bending the circuit board to generate spontaneous and unpredictable sounds. Databending takes a similar approach to circuit bending, using software to intentionally disrupt the information contained within a file. There\u0026rsquo;s all kinds of different techniques, some involving deep hex editing of certain parts of a compression algorithm, but other methods are surprisingly simple. A hex editor (or binary file editor or byte editor) is a computer program that allows for manipulation of the fundamental binary data that constitutes a computer file. As technology becomes more complex, more glitches occur. A new term, \u0026ldquo;glitch art\u0026rdquo; has been coined for these glitches. An increasing number of artists have started to value these glitches and have come to insert them intentionally in whatever they are trying to create though many argue that the term glitch art can only be used for glitches that are unintentional. The different techniques of databending can broadly be divided into three categories:\nIncorrect editing: In incorrect editing, a file is edited using software intended for a different type of data. Reinterpretation: Reinterpretation is where a file is converted from one medium to another. Field of forced errors: In this type of databending known bugs in programs are exploited to force them to fail, usually while writing a file in the hope that the written file will be corrupted. ","date":"2022-07-24T13:15:46+05:30","permalink":"https://jhaakansha.github.io/p/data-bending/","title":"Data Bending"},{"content":" Big O is the used to decribe the efficiency of algorithms. Academics use big O, big θ and big Ω to decribe runtimes.\nBig O Big Ω Big θ It describes upper bound on time It describes lower bound on time θ means both O and Ω An algorithm that requires O(n) time can also be described as requiring O(n²), O(n³), O(2ⁿ) etc. The algorithm is at least as fast as any of these If the runtime is decribed as Ω(n), then it can also be described by Ω(log n) and Ω(1). But, it won\u0026rsquo;t be faster than those runtimes That is, an algorithm is θ(n) if its both O(n) and Ω(n). θ gives a tight bound on runtimes In industry, the meaning of O and θ seem to have been merged. Industry\u0026rsquo;s meaning of big O is closer to what academics mean by θ. The runtime for an algorithm is described in three different ways:\nBest Case : If we consider the case of sorting an array, then if all the elements are equal, then quick sort will traverse the array only once giving the runtime O(n). This is the least runtime possible, hence it is the best case scenario. Worst Case : If we get really unlucky and the pivot is repeatedly the biggest element in the array then we have the largest possible runtime. This is the worst case scenario. Expected Case : Usually, the conditions considered above don\u0026rsquo;t happen. In general cases, the expected runtime for quick sort will be O(n log n). ","date":"2022-07-14T12:09:29+05:30","permalink":"https://jhaakansha.github.io/p/big-o-notation/","title":"Big O Notation"}]